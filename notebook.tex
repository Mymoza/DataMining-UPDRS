
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{UPDRS}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{MTI830}\label{mti830}

    Projet de session

Équipe: - Marie-Philippe Gill - Félix Blier

    \subsection{Data}\label{data}

    The dataset was first introduced by
\href{https://onlinelibrary.wiley.com/doi/full/10.1002/mds.22379}{{[}2{]}
Goetz et al.}, and the dataset was shared by
\href{https://ieeexplore.ieee.org/abstract/document/5339170}{{[}1{]}
Tsanas et al.} on
\href{https://data.world/uci/parkinsons-telemonitoring/workspace/project-summary?agentid=uci\&datasetid=parkinsons-telemonitoring}{data.world}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{pylab}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{TruncatedSVD}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{cluster} \PY{k}{import} \PY{n}{KMeans}
        
        \PY{k+kn}{import} \PY{n+nn}{math}
        \PY{k+kn}{import} \PY{n+nn}{pandas}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{MinMaxScaler}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVR}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}\PY{p}{,} \PY{n}{cross\PYZus{}validate}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{shuffle}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{GridSearchCV}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{svm} \PY{k}{import} \PY{n}{SVR}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{https://query.data.world/s/pmiss3v44x4dmyrplzrsjo7coaehfc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{rename}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{subject\PYZsh{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{subject\PYZus{}id}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{df}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}2}]:}       subject\_id  age  sex  test\_time  motor\_UPDRS  total\_UPDRS  Jitter(\%)  \textbackslash{}
        0              1   72    0    5.64310       28.199       34.398    0.00662   
        1              1   72    0   12.66600       28.447       34.894    0.00300   
        2              1   72    0   19.68100       28.695       35.389    0.00481   
        3              1   72    0   25.64700       28.905       35.810    0.00528   
        4              1   72    0   33.64200       29.187       36.375    0.00335   
        5              1   72    0   40.65200       29.435       36.870    0.00353   
        6              1   72    0   47.64900       29.682       37.363    0.00422   
        7              1   72    0   54.64000       29.928       37.857    0.00476   
        8              1   72    0   61.66900       30.177       38.353    0.00432   
        9              1   72    0   68.68800       30.424       38.849    0.00496   
        10             1   72    0   75.65300       30.670       39.340    0.00465   
        11             1   72    0   82.65300       30.917       39.834    0.00537   
        12             1   72    0   89.63500       31.309       40.412    0.00524   
        13             1   72    0   96.63300       31.776       41.034    0.00354   
        14             1   72    0  103.64000       32.243       41.657    0.00530   
        15             1   72    0  110.65000       32.710       42.280    0.00456   
        16             1   72    0  117.66000       33.178       42.904    0.00693   
        17             1   72    0  124.64000       33.643       43.524    0.00652   
        18             1   72    0  131.64000       34.109       44.146    0.00571   
        19             1   72    0  139.69000       34.646       44.861    0.00372   
        20             1   72    0  145.64000       35.043       45.390    0.00285   
        21             1   72    0  152.64000       35.509       46.013    0.00629   
        22             1   72    0  159.64000       35.976       46.635    0.00375   
        23             1   72    0  174.66000       36.977       47.970    0.00386   
        24             1   72    0    5.64310       28.199       34.398    0.00348   
        25             1   72    0   12.66700       28.447       34.894    0.00950   
        26             1   72    0   19.68200       28.695       35.389    0.00401   
        27             1   72    0   25.64700       28.905       35.810    0.00340   
        28             1   72    0   33.64300       29.187       36.375    0.00317   
        29             1   72    0   40.65200       29.435       36.870    0.00471   
        {\ldots}          {\ldots}  {\ldots}  {\ldots}        {\ldots}          {\ldots}          {\ldots}        {\ldots}   
        5845          42   61    0  142.79000       22.485       33.485    0.00373   
        5846          42   61    0  149.84000       21.988       32.988    0.00233   
        5847          42   61    0  156.82000       21.495       32.495    0.00407   
        5848          42   61    0  163.73000       21.007       32.007    0.00250   
        5849          42   61    0  170.73000       20.513       31.513    0.00269   
        5850          42   61    0   -4.25970       19.725       28.634    0.00352   
        5851          42   61    0    0.39931       20.026       29.034    0.00742   
        5852          42   61    0    9.71390       20.627       29.836    0.00330   
        5853          42   61    0   16.71100       21.078       30.438    0.00239   
        5854          42   61    0   23.76100       21.533       31.044    0.00541   
        5855          42   61    0   30.64500       21.977       31.636    0.00203   
        5856          42   61    0   37.77200       22.437       32.249    0.00277   
        5857          42   61    0   44.64000       22.880       32.840    0.00549   
        5858          42   61    0   51.76200       23.339       33.453    0.00274   
        5859          42   61    0   58.76200       23.791       34.055    0.00541   
        5860          42   61    0   65.75700       24.242       34.657    0.00306   
        5861          42   61    0   79.77700       25.147       35.863    0.00335   
        5862          42   61    0   86.76700       25.598       36.464    0.00225   
        5863          42   61    0   93.54600       25.961       36.961    0.00492   
        5864          42   61    0  100.77000       25.452       36.452    0.00250   
        5865          42   61    0  106.76000       25.029       36.029    0.00329   
        5866          42   61    0  115.65000       24.401       35.401    0.00448   
        5867          42   61    0  121.63000       23.979       34.979    0.00250   
        5868          42   61    0  128.67000       23.482       34.482    0.00249   
        5869          42   61    0  136.80000       22.908       33.908    0.00274   
        5870          42   61    0  142.79000       22.485       33.485    0.00406   
        5871          42   61    0  149.84000       21.988       32.988    0.00297   
        5872          42   61    0  156.82000       21.495       32.495    0.00349   
        5873          42   61    0  163.73000       21.007       32.007    0.00281   
        5874          42   61    0  170.73000       20.513       31.513    0.00282   
        
              Jitter(Abs)  Jitter:RAP  Jitter:PPQ5    {\ldots}     Shimmer(dB)  \textbackslash{}
        0        0.000034     0.00401      0.00317    {\ldots}           0.230   
        1        0.000017     0.00132      0.00150    {\ldots}           0.179   
        2        0.000025     0.00205      0.00208    {\ldots}           0.181   
        3        0.000027     0.00191      0.00264    {\ldots}           0.327   
        4        0.000020     0.00093      0.00130    {\ldots}           0.176   
        5        0.000023     0.00119      0.00159    {\ldots}           0.214   
        6        0.000024     0.00212      0.00221    {\ldots}           0.445   
        7        0.000025     0.00226      0.00259    {\ldots}           0.212   
        8        0.000029     0.00156      0.00207    {\ldots}           0.371   
        9        0.000027     0.00258      0.00253    {\ldots}           0.310   
        10       0.000026     0.00238      0.00260    {\ldots}           0.188   
        11       0.000032     0.00236      0.00278    {\ldots}           0.282   
        12       0.000033     0.00235      0.00251    {\ldots}           0.240   
        13       0.000024     0.00142      0.00150    {\ldots}           0.171   
        14       0.000032     0.00241      0.00231    {\ldots}           0.291   
        15       0.000029     0.00152      0.00194    {\ldots}           0.264   
        16       0.000039     0.00329      0.00285    {\ldots}           0.274   
        17       0.000038     0.00313      0.00311    {\ldots}           0.320   
        18       0.000037     0.00296      0.00293    {\ldots}           0.223   
        19       0.000022     0.00181      0.00195    {\ldots}           0.288   
        20       0.000016     0.00079      0.00109    {\ldots}           0.133   
        21       0.000036     0.00278      0.00293    {\ldots}           0.338   
        22       0.000022     0.00157      0.00175    {\ldots}           0.244   
        23       0.000023     0.00178      0.00195    {\ldots}           0.295   
        24       0.000015     0.00124      0.00133    {\ldots}           0.113   
        25       0.000059     0.00446      0.00457    {\ldots}           0.411   
        26       0.000024     0.00149      0.00185    {\ldots}           0.160   
        27       0.000021     0.00178      0.00162    {\ldots}           0.157   
        28       0.000017     0.00128      0.00156    {\ldots}           0.114   
        29       0.000028     0.00165      0.00146    {\ldots}           0.226   
        {\ldots}           {\ldots}         {\ldots}          {\ldots}    {\ldots}             {\ldots}   
        5845     0.000030     0.00171      0.00154    {\ldots}           0.245   
        5846     0.000020     0.00101      0.00123    {\ldots}           0.150   
        5847     0.000030     0.00209      0.00222    {\ldots}           0.223   
        5848     0.000018     0.00102      0.00135    {\ldots}           0.163   
        5849     0.000021     0.00121      0.00132    {\ldots}           0.154   
        5850     0.000027     0.00125      0.00169    {\ldots}           0.159   
        5851     0.000065     0.00330      0.00339    {\ldots}           0.497   
        5852     0.000025     0.00135      0.00159    {\ldots}           0.108   
        5853     0.000018     0.00122      0.00136    {\ldots}           0.139   
        5854     0.000041     0.00285      0.00296    {\ldots}           0.317   
        5855     0.000016     0.00081      0.00105    {\ldots}           0.120   
        5856     0.000022     0.00109      0.00135    {\ldots}           0.148   
        5857     0.000047     0.00172      0.00176    {\ldots}           0.213   
        5858     0.000020     0.00119      0.00120    {\ldots}           0.154   
        5859     0.000044     0.00280      0.00199    {\ldots}           0.207   
        5860     0.000025     0.00140      0.00167    {\ldots}           0.167   
        5861     0.000029     0.00149      0.00175    {\ldots}           0.173   
        5862     0.000017     0.00101      0.00117    {\ldots}           0.125   
        5863     0.000039     0.00250      0.00177    {\ldots}           0.141   
        5864     0.000020     0.00117      0.00136    {\ldots}           0.137   
        5865     0.000024     0.00144      0.00132    {\ldots}           0.180   
        5866     0.000036     0.00208      0.00226    {\ldots}           0.213   
        5867     0.000019     0.00116      0.00136    {\ldots}           0.193   
        5868     0.000021     0.00066      0.00104    {\ldots}           0.142   
        5869     0.000021     0.00118      0.00138    {\ldots}           0.218   
        5870     0.000031     0.00167      0.00168    {\ldots}           0.160   
        5871     0.000025     0.00119      0.00147    {\ldots}           0.215   
        5872     0.000025     0.00152      0.00187    {\ldots}           0.244   
        5873     0.000020     0.00128      0.00151    {\ldots}           0.131   
        5874     0.000021     0.00135      0.00166    {\ldots}           0.171   
        
              Shimmer:APQ3  Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR  \textbackslash{}
        0          0.01438       0.01309        0.01662      0.04314  0.014290   
        1          0.00994       0.01072        0.01689      0.02982  0.011112   
        2          0.00734       0.00844        0.01458      0.02202  0.020220   
        3          0.01106       0.01265        0.01963      0.03317  0.027837   
        4          0.00679       0.00929        0.01819      0.02036  0.011625   
        5          0.01006       0.01337        0.02263      0.03019  0.009438   
        6          0.02376       0.02621        0.03488      0.07128  0.013260   
        7          0.00979       0.01462        0.01911      0.02937  0.027969   
        8          0.01774       0.02134        0.03451      0.05323  0.013381   
        9          0.02030       0.01970        0.02569      0.06089  0.018021   
        10         0.01069       0.01214        0.01844      0.03206  0.017443   
        11         0.01001       0.01375        0.02395      0.03003  0.017115   
        12         0.01176       0.01395        0.02019      0.03528  0.011876   
        13         0.00847       0.01040        0.01920      0.02540  0.015008   
        14         0.01310       0.01260        0.02069      0.03930  0.018093   
        15         0.01379       0.01494        0.02309      0.04138  0.020181   
        16         0.01468       0.01430        0.01952      0.04405  0.041980   
        17         0.01603       0.01733        0.02293      0.04810  0.031634   
        18         0.01260       0.01466        0.02145      0.03780  0.031546   
        19         0.01458       0.01732        0.02908      0.04373  0.010976   
        20         0.00567       0.00682        0.01299      0.01702  0.004652   
        21         0.01915       0.02174        0.03315      0.05745  0.043582   
        22         0.01120       0.01283        0.02063      0.03360  0.014068   
        23         0.01312       0.01514        0.02626      0.03936  0.015298   
        24         0.00411       0.00463        0.00949      0.01234  0.009238   
        25         0.01828       0.01899        0.01999      0.05484  0.052492   
        26         0.00623       0.00768        0.01039      0.01868  0.029589   
        27         0.00711       0.00765        0.00926      0.02132  0.016636   
        28         0.00474       0.00644        0.00920      0.01421  0.014619   
        29         0.00971       0.01166        0.01774      0.02914  0.032426   
        {\ldots}            {\ldots}           {\ldots}            {\ldots}          {\ldots}       {\ldots}   
        5845       0.01559       0.01373        0.01931      0.04677  0.022905   
        5846       0.00835       0.01008        0.01414      0.02504  0.005008   
        5847       0.01068       0.01320        0.01971      0.03204  0.019919   
        5848       0.00853       0.01033        0.01518      0.02558  0.005421   
        5849       0.00945       0.00968        0.01353      0.02836  0.009154   
        5850       0.00739       0.00914        0.01465      0.02216  0.009635   
        5851       0.02483       0.03028        0.04919      0.07450  0.053312   
        5852       0.00553       0.00681        0.01006      0.01659  0.009870   
        5853       0.00851       0.00969        0.01300      0.02553  0.004231   
        5854       0.01840       0.01989        0.02801      0.05520  0.026224   
        5855       0.00627       0.00786        0.01371      0.01882  0.004652   
        5856       0.00833       0.00932        0.01450      0.02499  0.006018   
        5857       0.00937       0.01171        0.01857      0.02810  0.030641   
        5858       0.00797       0.00956        0.01510      0.02392  0.008989   
        5859       0.01007       0.01228        0.01871      0.03021  0.041629   
        5860       0.00805       0.00885        0.01335      0.02416  0.019214   
        5861       0.00974       0.01152        0.01623      0.02922  0.017758   
        5862       0.00551       0.00604        0.00974      0.01654  0.019286   
        5863       0.00832       0.00915        0.01343      0.02495  0.021339   
        5864       0.00743       0.00958        0.01227      0.02228  0.007679   
        5865       0.01020       0.01287        0.01776      0.03059  0.010377   
        5866       0.01121       0.01475        0.01971      0.03362  0.035738   
        5867       0.01104       0.01316        0.01987      0.03313  0.012234   
        5868       0.00469       0.00773        0.01997      0.01406  0.007452   
        5869       0.01375       0.01595        0.02108      0.04126  0.006058   
        5870       0.00973       0.01133        0.01549      0.02920  0.025137   
        5871       0.01052       0.01277        0.01904      0.03157  0.011927   
        5872       0.01371       0.01456        0.01877      0.04112  0.017701   
        5873       0.00693       0.00870        0.01307      0.02078  0.007984   
        5874       0.00946       0.01154        0.01470      0.02839  0.008172   
        
                 HNR     RPDE      DFA       PPE  
        0     21.640  0.41888  0.54842  0.160060  
        1     27.183  0.43493  0.56477  0.108100  
        2     23.047  0.46222  0.54405  0.210140  
        3     24.445  0.48730  0.57794  0.332770  
        4     26.126  0.47188  0.56122  0.193610  
        5     22.946  0.53949  0.57243  0.195000  
        6     22.506  0.49250  0.54779  0.175630  
        7     22.929  0.47712  0.54234  0.238440  
        8     22.078  0.51563  0.61864  0.200370  
        9     22.606  0.50032  0.58673  0.201170  
        10    25.672  0.49892  0.61068  0.173870  
        11    24.204  0.46686  0.57984  0.193900  
        12    22.203  0.56600  0.60571  0.209840  
        13    24.614  0.61348  0.60661  0.158810  
        14    23.533  0.51577  0.56790  0.214610  
        15    22.203  0.51806  0.56978  0.175080  
        16    20.878  0.52874  0.57711  0.349480  
        17    22.212  0.50991  0.61093  0.230480  
        18    23.129  0.52714  0.59220  0.182110  
        19    22.939  0.49687  0.57726  0.165670  
        20    25.181  0.42536  0.54735  0.169460  
        21    20.757  0.58088  0.56681  0.279240  
        22    24.275  0.43119  0.56869  0.193390  
        23    24.126  0.43806  0.59755  0.201640  
        24    27.927  0.37340  0.52499  0.170660  
        25    20.533  0.55096  0.55348  0.260940  
        26    26.126  0.51888  0.54699  0.215500  
        27    25.986  0.42271  0.56963  0.117740  
        28    26.514  0.49559  0.53565  0.188910  
        29    25.188  0.49520  0.55286  0.228830  
        {\ldots}      {\ldots}      {\ldots}      {\ldots}       {\ldots}  
        5845  23.123  0.51492  0.55169  0.113440  
        5846  25.667  0.46075  0.55979  0.112750  
        5847  23.695  0.48543  0.57984  0.181380  
        5848  24.712  0.40744  0.58011  0.133990  
        5849  25.280  0.49084  0.58159  0.120550  
        5850  24.315  0.58904  0.57961  0.162960  
        5851  19.091  0.65203  0.60188  0.310670  
        5852  26.380  0.53722  0.57262  0.132510  
        5853  26.274  0.50529  0.57319  0.115000  
        5854  21.415  0.45038  0.60756  0.235260  
        5855  26.989  0.52472  0.55385  0.116240  
        5856  24.639  0.61557  0.57417  0.140920  
        5857  23.897  0.61667  0.58380  0.120380  
        5858  25.014  0.59282  0.56452  0.115130  
        5859  24.307  0.55461  0.56013  0.111350  
        5860  24.689  0.55307  0.57152  0.112290  
        5861  23.295  0.60473  0.57538  0.137330  
        5862  27.996  0.54585  0.53726  0.099762  
        5863  24.727  0.59338  0.55640  0.210690  
        5864  23.835  0.54909  0.55672  0.119140  
        5865  24.873  0.51983  0.56668  0.113820  
        5866  21.252  0.62957  0.57942  0.245670  
        5867  20.661  0.47256  0.57177  0.139550  
        5868  25.258  0.52976  0.54378  0.088268  
        5869  23.935  0.45357  0.56142  0.134460  
        5870  22.369  0.64215  0.55314  0.213670  
        5871  22.886  0.52598  0.56518  0.126210  
        5872  25.065  0.47792  0.57888  0.141570  
        5873  24.422  0.56865  0.56327  0.142040  
        5874  23.259  0.58608  0.57077  0.153360  
        
        [5875 rows x 22 columns]
\end{Verbatim}
            
    \subsection{Objective 1: Data
understanding}\label{objective-1-data-understanding}

    

    \begin{itemize}
\tightlist
\item
  \texttt{Subject\#} : Subject id
\item
  \texttt{age}: Age
\item
  \texttt{test\_time}: Time since recruitment into the trial. The
  integer part is the number of days since recruitment.
\end{itemize}

The data use UPDRS scores as the gold standart. You can see an example
of UPDRS questionnaire
\href{https://img.medscape.com/fullsize/701/816/58977_UPDRS.pdf}{here}.

\begin{itemize}
\item
  \texttt{motor\_UPDRS}: Clinician's motor UPDRS score, linearly
  interpolated. It refers to UPDRS-III, which are questions 18 to 31 in
  the UPDRS test. The minimum of points is 0 and maximum is 56 as the
  answers range from 0 to 4 points.
\item
  \texttt{total\_UPDRS}: Clinician's total UPDRS score, linearly
  interpolated. The total points of UPDRS can go from 0 to 199 points.
\end{itemize}

The 16 following are biomedical voice measures. {[}1{]}

Jitter : Cycle-to-cycle variability in F0

\begin{itemize}
\tightlist
\item
  \texttt{Jitter(\%)} : Jitter as a percentage
\item
  \texttt{Jitter(Abs)}: Absolute jitter in microseconds
\item
  \texttt{Jitter:RAP}: Relative amplitude perturbation
\item
  \texttt{Jitter:PPQ5}: Five point period perturbation quotient
\item
  \texttt{Jitter:DDP}: Differences between cycles, divided by the
  average periode
\end{itemize}

Shimmer : Cycle-to-cycle variability amplitude

\begin{itemize}
\tightlist
\item
  \texttt{Shimmer}: Local shimemr
\item
  \texttt{Shimmer(dB)}: Local shimemr in decibels
\item
  \texttt{Shimmer:APQ3}: Three point Amplitude Perturbation Quotient
\item
  \texttt{Shimmer:APQ5}: TFive point Amplitude Perturbation Quotient
\item
  \texttt{Shimmer:APQ11}: 11-point Amplitude Perturbation Quotient
\item
  \texttt{Shimmer:DDA}: Difference between the amplitudes od consecutive
  periods
\end{itemize}

HArmonic and noise ratio - \texttt{NHR}: Harmonics-to-noise ratio -
\texttt{HNR}: Noise-to-harmonics ratio

Speech signal processing methods - \texttt{RPDE}: Recurence period
density entropy is the ability of the vocal folds to sustain simple
vibration - \texttt{DFA}: Detrended fluctuation analysis is the extent
of turbulent noise in the speech signal - \texttt{PPE}: Pitch period
entropy is the impaired control of stable pitch during sustained
phonation

    There were 5,875 voice recordings. The aim of the provided dataset is to
predict the motor and total UPDRS score from the 16 voice measures.

Recordings are made at home using an At-Home Testing Device (AHTD).

The speech test is designed to take 4 minutes to execute.

The test is executed weekly.

The speech test is composed of three processes : - 4 decay of normal
intensity phonation for maximally sustained ``Ah'' phonation {[}1{]} - 2
decay of loud intensity (twice the normal intensity) phonation for
maximallysustained ``Ah'' {[}1{]} - Intensity decays for descriptions of
standardized pictures with and without ﬁnger tap-ping motor distraction.
{[}2{]}

The patients in this study were in early-stage and unmedicated
Parkinson's Disease patients.

52 patients were enrolled in the study, but only 50 completed the
6-month period. From those, 48 remained without medication.

The initial study reported "UPDRS motor scores significantly worsened
over the 6 months" {[}2{]}.

    \subsubsection{Questions à répondre}\label{questions-uxe0-ruxe9pondre}

\begin{itemize}
\tightlist
\item
  C'est quoi "linaerly interpolated', je comprends pas comment ça a été
  utilisé. Est-ce dans le modèle de régression utilisé par l'étude? : Ça
  signifie que le score motor UPDRS est proportiellement linéaire à la
  gravité de la maladie du patient (un patient ayant un score UPDRS de
  fois plus levé sera 2 fois plus touché par la maladie qu'un autre
  patient).
\end{itemize}

    \subsection{Data Visualization}\label{data-visualization}

    \subsubsection{Exploration des attributs et de leurs
valeurs}\label{exploration-des-attributs-et-de-leurs-valeurs}

    \begin{itemize}
\tightlist
\item
  Attributs qualitatifs : nominal, binaire, ordinal ?
\item
  Quantitatif : discret, continu ?
\end{itemize}

Qualitatif : - Binaire : sex - Nominal : subject\_id, age Discrètes : -
discret : tous les autres attributs

    \subsubsection{Mesures de tendance
centrale}\label{mesures-de-tendance-centrale}

    Comme mentionné dans le cours 4, il est toujours intéressant de
s'informer sur notre dataset en utilisant des mesures de tendances
centrales, en utilisant par exemple : - Moyenne - médiane - mode -
mid-range (moyenne entre le min et le max) - variance - écart type

Comme notre dataset est contenu dans un \texttt{DataFrame}, en python,
on peut utiliser l'attribut \texttt{.describe()} pour obtenir ces
informations sur les features de notre dataset.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{df}\PY{o}{.}\PY{n}{sex}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:} 0    4008
        1    1867
        Name: sex, dtype: int64
\end{Verbatim}
            
    La base de donnée contien 4008 enregistrements masculins et 1867
enregistrement féminins

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:}         subject\_id          age          sex    test\_time  motor\_UPDRS  \textbackslash{}
        count  5875.000000  5875.000000  5875.000000  5875.000000  5875.000000   
        mean     21.494128    64.804936     0.317787    92.863722    21.296229   
        std      12.372279     8.821524     0.465656    53.445602     8.129282   
        min       1.000000    36.000000     0.000000    -4.262500     5.037700   
        25\%      10.000000    58.000000     0.000000    46.847500    15.000000   
        50\%      22.000000    65.000000     0.000000    91.523000    20.871000   
        75\%      33.000000    72.000000     1.000000   138.445000    27.596500   
        max      42.000000    85.000000     1.000000   215.490000    39.511000   
        
               total\_UPDRS    Jitter(\%)  Jitter(Abs)   Jitter:RAP  Jitter:PPQ5  \textbackslash{}
        count  5875.000000  5875.000000  5875.000000  5875.000000  5875.000000   
        mean     29.018942     0.006154     0.000044     0.002987     0.003277   
        std      10.700283     0.005624     0.000036     0.003124     0.003732   
        min       7.000000     0.000830     0.000002     0.000330     0.000430   
        25\%      21.371000     0.003580     0.000022     0.001580     0.001820   
        50\%      27.576000     0.004900     0.000035     0.002250     0.002490   
        75\%      36.399000     0.006800     0.000053     0.003290     0.003460   
        max      54.992000     0.099990     0.000446     0.057540     0.069560   
        
                  {\ldots}       Shimmer(dB)  Shimmer:APQ3  Shimmer:APQ5  Shimmer:APQ11  \textbackslash{}
        count     {\ldots}       5875.000000   5875.000000   5875.000000    5875.000000   
        mean      {\ldots}          0.310960      0.017156      0.020144       0.027481   
        std       {\ldots}          0.230254      0.013237      0.016664       0.019986   
        min       {\ldots}          0.026000      0.001610      0.001940       0.002490   
        25\%       {\ldots}          0.175000      0.009280      0.010790       0.015665   
        50\%       {\ldots}          0.253000      0.013700      0.015940       0.022710   
        75\%       {\ldots}          0.365000      0.020575      0.023755       0.032715   
        max       {\ldots}          2.107000      0.162670      0.167020       0.275460   
        
               Shimmer:DDA          NHR          HNR         RPDE          DFA  \textbackslash{}
        count  5875.000000  5875.000000  5875.000000  5875.000000  5875.000000   
        mean      0.051467     0.032120    21.679495     0.541473     0.653240   
        std       0.039711     0.059692     4.291096     0.100986     0.070902   
        min       0.004840     0.000286     1.659000     0.151020     0.514040   
        25\%       0.027830     0.010955    19.406000     0.469785     0.596180   
        50\%       0.041110     0.018448    21.920000     0.542250     0.643600   
        75\%       0.061735     0.031463    24.444000     0.614045     0.711335   
        max       0.488020     0.748260    37.875000     0.966080     0.865600   
        
                       PPE  
        count  5875.000000  
        mean      0.219589  
        std       0.091498  
        min       0.021983  
        25\%       0.156340  
        50\%       0.205500  
        75\%       0.264490  
        max       0.731730  
        
        [8 rows x 22 columns]
\end{Verbatim}
            
    Ainsi, on peut constater que notre dataset contient 5875 enregistrements
pour 42 patients différents.

Ceux-ci ont en moyenne 64.8 ans. Le patient le plus jeune a 36 ans, et
le plus vieux a 85 ans.

La moyenne d'age est assez près du entre qui est 60.5 ans. L'écart type
est quand à elle assez élevée.

Il est intéressant de visualiser la distribution de l'age des patients
grace a un histogramme.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{df}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{52}\PY{p}{,}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reccord}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age histograme of records}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Dans l'histogramme, l'axe des abscisse représente l'age du patient et
l'axe des ordonnées, le nombre d'enregistrement.

On observe ici que la plupart des patients ont pres de 58 ans, 65 ans et
75 ans.

L'histogramme des résultats des test UPDRS donne aussi une idée sur
l'etat des patients

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{df}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{52}\PY{p}{)}
        
        \PY{n}{df}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{52}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} array([[<matplotlib.axes.\_subplots.AxesSubplot object at 0x000001B1BD35D908>]],
              dtype=object)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Dans l'histogramme, l'axe des abscisse représente l'age du patient et
l'axe des ordonnées, le nombre d'enregistrement.

Le score total de UPDRS semble suivre une loi normale de moyenne 30

    \subsection{QQ Plot}\label{qq-plot}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
        \PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm} 
        \PY{k+kn}{import} \PY{n+nn}{pylab} \PY{k}{as} \PY{n+nn}{py} 
        
        \PY{n}{sm}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
        \PY{n}{py}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Probability Plot of the motor\PYZus{}UPDRS score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{sm}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
        \PY{n}{py}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Probability Plot of the total\PYZus{}UPDRS score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{py}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)} 
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{sm}\PY{o}{.}\PY{n}{qqplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{line}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{s}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
        \PY{n}{py}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{QQPlot of the total\PYZus{}UPDRS score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{py}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)} 
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Bloxplot - Données
aberrantes}\label{bloxplot---donnuxe9es-aberrantes}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Index(['subject\_id', 'age', 'sex', 'test\_time', 'motor\_UPDRS', 'total\_UPDRS',
       'Jitter(\%)', 'Jitter(Abs)', 'Jitter:RAP', 'Jitter:PPQ5', 'Jitter:DDP',
       'Shimmer', 'Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',
       'Shimmer:APQ11', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'PPE'],
      dtype='object')

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{x1} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{age}
         \PY{n}{x2} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{test\PYZus{}time}
         
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x1}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}data = [x1, x2, x3, x4, x5, x6, x7, x8, x9]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Boxplot Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} ([<matplotlib.axis.XTick at 0x1b1bde797f0>],
          <a list of 1 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{x1} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{age}
         \PY{n}{x2} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{test\PYZus{}time}
         
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x2}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}data = [x1, x2, x3, x4, x5, x6, x7, x8, x9]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} ([<matplotlib.axis.XTick at 0x1b1bddddac8>],
          <a list of 1 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{x1} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}
         \PY{n}{x2} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}
         
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{x1}\PY{p}{,}\PY{n}{x2}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}data = [x1, x2, x3, x4, x5, x6, x7, x8, x9]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{UPDRS scores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Motor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Total}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} ([<matplotlib.axis.XTick at 0x1b1bd431e48>,
           <matplotlib.axis.XTick at 0x1b1bd431198>],
          <a list of 2 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Schimmer}
         
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer(dB)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer:APQ3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer:APQ5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer:APQ11}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer:DDA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}data = [x1, x2, x3, x4, x5, x6, x7, x8, x9]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Schimmer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shimmer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S (dB)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S :APQ3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S :APQ5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S :APQ11}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{S :DDA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} ([<matplotlib.axis.XTick at 0x1b1bcef0ac8>,
           <matplotlib.axis.XTick at 0x1b1bd46f1d0>,
           <matplotlib.axis.XTick at 0x1b1bd46f668>,
           <matplotlib.axis.XTick at 0x1b1bd3175c0>,
           <matplotlib.axis.XTick at 0x1b1bd317a90>,
           <matplotlib.axis.XTick at 0x1b1bd317f60>],
          <a list of 6 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Jitter}
         
         \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter(}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter(Abs)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:RAP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:PPQ5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:DDP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter(}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter(Abs)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:RAP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:PPQ5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter:DDP}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} ([<matplotlib.axis.XTick at 0x1b1bdded400>,
           <matplotlib.axis.XTick at 0x1b1bddfbcf8>,
           <matplotlib.axis.XTick at 0x1b1bddfba58>,
           <matplotlib.axis.XTick at 0x1b1bdd95320>,
           <matplotlib.axis.XTick at 0x1b1bdd957f0>],
          <a list of 5 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{data} \PY{o}{=} \PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HNR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RPDE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DFA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PPE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
         
         \PY{n}{fig1}\PY{p}{,} \PY{n}{ax1} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Jitter}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax1}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{data}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NHR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{HNR}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RPDE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{DFA}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PPE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} ([<matplotlib.axis.XTick at 0x1b1bde8edd8>,
           <matplotlib.axis.XTick at 0x1b1bde8e748>,
           <matplotlib.axis.XTick at 0x1b1bde8e4a8>,
           <matplotlib.axis.XTick at 0x1b1bdeb2cf8>,
           <matplotlib.axis.XTick at 0x1b1bdebc208>],
          <a list of 5 Text xticklabel objects>)
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_34_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Motor UPDRS Score Over time since recruitment in the
study for the
subjects}\label{motor-updrs-score-over-time-since-recruitment-in-the-study-for-the-subjects}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} Source of the code for the subplots:}
         \PY{c+c1}{\PYZsh{} https://matplotlib.org/3.1.1/gallery/lines\PYZus{}bars\PYZus{}and\PYZus{}markers/markevery\PYZus{}demo.html\PYZsh{}sphx\PYZhy{}glr\PYZhy{}gallery\PYZhy{}lines\PYZhy{}bars\PYZhy{}and\PYZhy{}markers\PYZhy{}markevery\PYZhy{}demo\PYZhy{}py}
         
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{gridspec} \PY{k}{as} \PY{n+nn}{gridspec}
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{matplotlib}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}version\PYZus{}\PYZus{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} define the figure size and grid layout properties}
         \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{17}\PY{p}{,}\PY{l+m+mi}{18}\PY{p}{)}
         \PY{n}{cols} \PY{o}{=} \PY{l+m+mi}{3}
         \PY{n}{rows} \PY{o}{=} \PY{l+m+mi}{14}
         \PY{c+c1}{\PYZsh{} define the data for cartesian plots}
         \PY{c+c1}{\PYZsh{} delta = 0.11}
         \PY{c+c1}{\PYZsh{} x = np.linspace(0, 10 \PYZhy{} 2 * delta, 200) + delta}
         \PY{c+c1}{\PYZsh{} y = np.sin(x) + 1.0 + delta}
         
         
         \PY{k}{def} \PY{n+nf}{trim\PYZus{}axs}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}little helper to massage the axs list to have correct length...\PYZdq{}\PYZdq{}\PYZdq{}}
             \PY{n}{axs} \PY{o}{=} \PY{n}{axs}\PY{o}{.}\PY{n}{flat}
             \PY{k}{for} \PY{n}{ax} \PY{o+ow}{in} \PY{n}{axs}\PY{p}{[}\PY{n}{N}\PY{p}{:}\PY{p}{]}\PY{p}{:}
                 \PY{n}{ax}\PY{o}{.}\PY{n}{remove}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{axs}\PY{p}{[}\PY{p}{:}\PY{n}{N}\PY{p}{]}
         
         \PY{n}{fig2}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{n}{figsize}\PY{p}{,} \PY{n}{constrained\PYZus{}layout}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{axs} \PY{o}{=} \PY{n}{trim\PYZus{}axs}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{l+m+mi}{42}\PY{p}{)}
         \PY{k}{for} \PY{n}{ax}\PY{p}{,} \PY{n}{subject\PYZus{}id} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id =}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{str}\PY{p}{(}\PY{n}{subject\PYZus{}id}\PY{p}{)}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}xscale(\PYZsq{}log\PYZsq{})}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}yscale(\PYZsq{}log\PYZsq{})}
             \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{test\PYZus{}time}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}\PY{p}{)}\PY{p}{)}
         \PY{n}{fig2}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}updrs\PYZus{}time\PYZus{}test.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2.2.2

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The next plots are showing for every subject, their motor UPDRS over the
time since they were recruited into the trial. The X axis represents the
number of days since the recruitment (\texttt{test\_time}) and the Y
axis is their \texttt{motor\_UPDRS}.

At first, I would have expected the motor UPDRS score to worsen over
time as this is what would happen in a degenerative disease. However,
****REF**** noted that for some patients, surprisingly, their motor
UPDRS is actually worsening in the middle of the experiments (at the 3
months visit) and then improving tremendously at the 6-months evaluation
of the UPDRS score.

Generating those graphs makes me want to go further in this analysis and
actually quantify how many patients:

\begin{itemize}
\tightlist
\item
  How many patients worsen from the beginning to the end of the study?
\item
  How many improved ?
\end{itemize}

    TODO FÉlix: As-tu une idée pourquoi un patient aurait deux fois la même
valeur dans test\_time? Le patient 1 a deux fois la même valeur minimum.
Par contre, les features vocales n'ont pas les mêmes valeurs

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{nb\PYZus{}subjects\PYZus{}worsen} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{nb\PYZus{}subjects\PYZus{}improved} \PY{o}{=} \PY{l+m+mi}{0} 
        \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well} \PY{o}{=} \PY{l+m+mi}{0} 
        
        \PY{n}{df\PYZus{}testtime\PYZus{}filtered} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
        
        \PY{c+c1}{\PYZsh{} Go through all the 42 subject\PYZus{}id of the dataset }
        \PY{k}{for} \PY{n}{subject\PYZus{}id} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}     print(\PYZsq{}Subject\PYZus{}id : \PYZsq{}, subject\PYZus{}id)}
            \PY{c+c1}{\PYZsh{} Filter the dataframe to only the recordings of the current subject }
            \PY{n}{subject\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{)}\PY{p}{]}
        
            \PY{c+c1}{\PYZsh{} Then we will filter another time the subject\PYZus{}df because we want the value of the motor UPDRS}
            \PY{c+c1}{\PYZsh{} when test\PYZus{}time is at its minimum, median and max value}
            \PY{n}{min\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}display(min\PYZus{}subject\PYZus{}df)}
            \PY{n}{df\PYZus{}testtime\PYZus{}filtered}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{min\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{min\PYZus{}value} \PY{o}{=} \PY{n}{min\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Min : \PYZsq{}, min\PYZus{}value)}
        
            \PY{c+c1}{\PYZsh{} Median uses row nearest to midpoint because otherwise median is a problem on dataframes of even length}
            \PY{n}{median\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}display(median\PYZus{}subject\PYZus{}df)}
            \PY{n}{df\PYZus{}testtime\PYZus{}filtered}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{median\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{median\PYZus{}value} \PY{o}{=} \PY{n}{median\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Median : \PYZsq{}, median\PYZus{}value)}
            
            \PY{n}{max\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{n}{df\PYZus{}testtime\PYZus{}filtered}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{max\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}display(max\PYZus{}subject\PYZus{}df)}
            \PY{n}{max\PYZus{}value} \PY{o}{=} \PY{n}{max\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Max : \PYZsq{}, max\PYZus{}value)}
            
            \PY{k}{if}\PY{p}{(}\PY{n}{min\PYZus{}value} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}worsen} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{elif}\PY{p}{(}\PY{n}{max\PYZus{}value} \PY{o}{\PYZlt{}} \PY{n}{min\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}improved} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{if}\PY{p}{(}\PY{n}{median\PYZus{}value} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who got worst between start and the end of the trial: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}worsen}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who are even better than when they started the trial : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}improved}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who improved between midpoint and end : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Nb subjects who got worst between start and the end of the trial:  27
Nb subjects who are even better than when they started the trial :  15
Nb subjects who improved between midpoint and end :  17

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{df\PYZus{}testtime\PYZus{}filtered}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}       subject\_id   age  sex  test\_time  motor\_UPDRS  total\_UPDRS  Jitter(\%)  \textbackslash{}
         0            1.0  72.0  0.0    5.64310       28.199       34.398    0.00662   
         12           1.0  72.0  0.0   89.63500       31.309       40.412    0.00524   
         23           1.0  72.0  0.0  174.66000       36.977       47.970    0.00386   
         149          2.0  58.0  0.0    3.86600       11.078       14.039    0.00600   
         235          2.0  58.0  0.0   90.80100       12.834       14.917    0.00524   
         173          2.0  58.0  0.0  178.80000       18.819       21.650    0.00440   
         294          3.0  57.0  0.0    7.28820       23.437       25.729    0.00565   
         306          3.0  57.0  0.0   91.30200       28.478       34.130    0.00293   
         317          3.0  57.0  0.0  168.27000       28.011       39.947    0.00199   
         438          4.0  74.0  0.0    9.68680       10.737       15.263    0.00410   
         517          4.0  74.0  0.0   92.55300       17.012       26.012    0.00314   
         460          4.0  74.0  0.0  170.76000       18.770       27.770    0.00820   
         600          5.0  75.0  0.0    7.35970       31.000       39.240    0.00645   
         579          5.0  75.0  0.0   84.38900       31.000       41.752    0.00352   
         591          5.0  75.0  0.0  168.39000       33.761       44.761    0.00730   
         731          6.0  63.0  0.0    5.34100       27.883       39.059    0.00961   
         744          6.0  63.0  0.0   96.34100       26.237       40.356    0.00394   
         834          6.0  63.0  0.0  180.34000       29.971       45.956    0.00774   
         906          7.0  72.0  0.0    7.25830       15.234       23.156    0.00925   
         973          7.0  72.0  0.0   84.30300       17.719       24.813    0.00524   
         905          7.0  72.0  0.0  175.30000       13.042       19.051    0.01214   
         1048         8.0  73.0  1.0    8.59510       18.093       24.093    0.00508   
         1110         8.0  73.0  1.0   92.65200       19.029       25.029    0.00343   
         1097         8.0  73.0  1.0  183.70000       23.031       29.031    0.00440   
         1198         9.0  68.0  0.0    4.43330       17.000       23.096    0.00821   
         1211         9.0  68.0  0.0  105.35000       17.607       25.243    0.01392   
         1223         9.0  68.0  0.0  196.36000       21.743       26.897    0.01032   
         1350        10.0  58.0  0.0   10.30300       12.000       19.000    0.00457   
         1436        10.0  58.0  0.0  101.36000       12.534       19.320    0.00607   
         1472        10.0  58.0  0.0  185.49000       16.870       21.922    0.00432   
         {\ldots}          {\ldots}   {\ldots}  {\ldots}        {\ldots}          {\ldots}          {\ldots}        {\ldots}   
         4396        33.0  66.0  1.0    9.38060       23.326       27.408    0.00733   
         4407        33.0  66.0  1.0  100.37000       26.491       31.364    0.00756   
         4418        33.0  66.0  1.0  177.46000       29.797       32.000    0.00340   
         4531        34.0  59.0  0.0   -3.31250       29.291       34.146    0.00441   
         4570        34.0  59.0  0.0   87.58400       21.300       30.150    0.00434   
         4610        34.0  59.0  0.0  178.68000       26.781       34.817    0.00377   
         4692        35.0  71.0  0.0    6.44030       36.073       54.073    0.00512   
         4706        35.0  71.0  0.0  104.50000       36.591       54.727    0.00737   
         4718        35.0  71.0  0.0  202.43000       34.163       53.109    0.00541   
         4857        36.0  62.0  1.0    8.33820       19.656       26.562    0.08034   
         4930        36.0  62.0  1.0   79.32800       25.239       31.348    0.03507   
         4878        36.0  62.0  1.0  170.34000       22.539       32.000    0.03288   
         4986        37.0  56.0  1.0    7.86880       22.962       32.874    0.00311   
         5044        37.0  56.0  1.0   98.84700       33.481       42.673    0.00507   
         5009        37.0  56.0  1.0  175.82000       37.664       48.530    0.00668   
         5126        38.0  67.0  0.0    7.77430       18.256       23.513    0.00544   
         5213        38.0  67.0  0.0   98.90700       20.839       28.758    0.00699   
         5150        38.0  67.0  0.0  181.89000       19.145       26.218    0.00872   
         5275        39.0  66.0  0.0    0.49861       25.033       34.044    0.00509   
         5405        39.0  66.0  0.0   75.39100       29.971       40.628    0.00471   
         5417        39.0  66.0  0.0  160.70000       34.064       44.298    0.00471   
         5418        40.0  85.0  1.0    7.73680       15.255       26.170    0.01258   
         5429        40.0  85.0  1.0   98.74200       17.745       27.490    0.00575   
         5440        40.0  85.0  1.0  176.71000       15.174       22.349    0.00561   
         5560        41.0  68.0  1.0    2.48610       29.211       37.211    0.00835   
         5683        41.0  68.0  1.0   91.52600       36.771       44.771    0.00726   
         5576        41.0  68.0  1.0  189.65000       33.643       42.515    0.00723   
         5725        42.0  61.0  0.0   -4.26250       19.725       28.633    0.01533   
         5762        42.0  61.0  0.0   86.76500       25.598       36.464    0.00561   
         5749        42.0  61.0  0.0  170.73000       20.513       31.513    0.00436   
         
               Jitter(Abs)  Jitter:RAP  Jitter:PPQ5   {\ldots}     Shimmer(dB)  \textbackslash{}
         0        0.000034     0.00401      0.00317   {\ldots}           0.230   
         12       0.000033     0.00235      0.00251   {\ldots}           0.240   
         23       0.000023     0.00178      0.00195   {\ldots}           0.295   
         149      0.000048     0.00302      0.00294   {\ldots}           0.331   
         235      0.000041     0.00276      0.00296   {\ldots}           0.351   
         173      0.000033     0.00230      0.00282   {\ldots}           0.437   
         294      0.000031     0.00321      0.00325   {\ldots}           0.246   
         306      0.000023     0.00135      0.00178   {\ldots}           0.118   
         317      0.000013     0.00087      0.00115   {\ldots}           0.109   
         438      0.000034     0.00163      0.00235   {\ldots}           0.273   
         517      0.000027     0.00142      0.00179   {\ldots}           0.275   
         460      0.000064     0.00429      0.00379   {\ldots}           0.517   
         600      0.000051     0.00244      0.00375   {\ldots}           0.299   
         579      0.000029     0.00138      0.00195   {\ldots}           0.272   
         591      0.000056     0.00381      0.00412   {\ldots}           0.403   
         731      0.000070     0.00549      0.00504   {\ldots}           0.385   
         744      0.000030     0.00183      0.00214   {\ldots}           0.424   
         834      0.000061     0.00414      0.00417   {\ldots}           0.426   
         906      0.000103     0.00523      0.00522   {\ldots}           0.251   
         973      0.000060     0.00219      0.00307   {\ldots}           0.279   
         905      0.000122     0.00566      0.00619   {\ldots}           0.558   
         1048     0.000027     0.00268      0.00279   {\ldots}           0.424   
         1110     0.000017     0.00158      0.00174   {\ldots}           0.201   
         1097     0.000023     0.00232      0.00245   {\ldots}           0.244   
         1198     0.000078     0.00395      0.00479   {\ldots}           0.310   
         1211     0.000141     0.00652      0.00550   {\ldots}           0.364   
         1223     0.000113     0.00519      0.00653   {\ldots}           0.225   
         1350     0.000041     0.00125      0.00206   {\ldots}           0.168   
         1436     0.000054     0.00209      0.00356   {\ldots}           0.273   
         1472     0.000034     0.00159      0.00193   {\ldots}           0.212   
         {\ldots}           {\ldots}         {\ldots}          {\ldots}   {\ldots}             {\ldots}   
         4396     0.000034     0.00409      0.00386   {\ldots}           0.486   
         4407     0.000042     0.00341      0.00361   {\ldots}           0.528   
         4418     0.000014     0.00141      0.00173   {\ldots}           0.251   
         4531     0.000037     0.00202      0.00239   {\ldots}           0.178   
         4570     0.000046     0.00173      0.00237   {\ldots}           0.105   
         4610     0.000038     0.00184      0.00223   {\ldots}           0.097   
         4692     0.000033     0.00186      0.00244   {\ldots}           0.263   
         4706     0.000049     0.00303      0.00433   {\ldots}           0.641   
         4718     0.000037     0.00232      0.00315   {\ldots}           0.486   
         4857     0.000391     0.04522      0.05689   {\ldots}           1.752   
         4930     0.000167     0.02002      0.02461   {\ldots}           1.655   
         4878     0.000206     0.01549      0.02366   {\ldots}           1.546   
         4986     0.000015     0.00149      0.00153   {\ldots}           0.471   
         5044     0.000025     0.00272      0.00247   {\ldots}           0.332   
         5009     0.000033     0.00399      0.00376   {\ldots}           0.263   
         5126     0.000030     0.00278      0.00238   {\ldots}           0.344   
         5213     0.000054     0.00282      0.00393   {\ldots}           0.325   
         5150     0.000056     0.00385      0.00413   {\ldots}           0.300   
         5275     0.000034     0.00238      0.00217   {\ldots}           0.231   
         5405     0.000032     0.00228      0.00248   {\ldots}           0.158   
         5417     0.000030     0.00193      0.00234   {\ldots}           0.238   
         5418     0.000081     0.00762      0.00721   {\ldots}           0.398   
         5429     0.000038     0.00319      0.00269   {\ldots}           0.317   
         5440     0.000059     0.00196      0.00291   {\ldots}           0.485   
         5560     0.000043     0.00444      0.00431   {\ldots}           0.290   
         5683     0.000041     0.00351      0.00325   {\ldots}           0.300   
         5576     0.000039     0.00317      0.00326   {\ldots}           0.235   
         5725     0.000118     0.00774      0.00800   {\ldots}           0.908   
         5762     0.000048     0.00232      0.00192   {\ldots}           0.278   
         5749     0.000037     0.00181      0.00196   {\ldots}           0.235   
         
               Shimmer:APQ3  Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR  \textbackslash{}
         0          0.01438       0.01309        0.01662      0.04314  0.014290   
         12         0.01176       0.01395        0.02019      0.03528  0.011876   
         23         0.01312       0.01514        0.02626      0.03936  0.015298   
         149        0.02058       0.02464        0.02778      0.06175  0.019901   
         235        0.02406       0.02581        0.03068      0.07217  0.022466   
         173        0.02352       0.03349        0.03659      0.07056  0.014770   
         294        0.01442       0.01341        0.01500      0.04326  0.029994   
         306        0.00631       0.00788        0.01125      0.01892  0.006619   
         317        0.00573       0.00707        0.00952      0.01720  0.004082   
         438        0.01378       0.01821        0.03138      0.04134  0.021982   
         517        0.01460       0.01782        0.02457      0.04379  0.028888   
         460        0.03870       0.03491        0.04670      0.11610  0.055807   
         600        0.01862       0.01927        0.02310      0.05586  0.038269   
         579        0.01681       0.01884        0.02441      0.05043  0.013082   
         591        0.02501       0.02568        0.03027      0.07503  0.061746   
         731        0.02545       0.02375        0.03089      0.07636  0.083288   
         744        0.02517       0.03066        0.04398      0.07550  0.010132   
         834        0.02553       0.02861        0.03624      0.07658  0.026459   
         906        0.01512       0.01848        0.02550      0.04537  0.040917   
         973        0.01365       0.01683        0.02661      0.04094  0.024570   
         905        0.02960       0.02643        0.04478      0.08880  0.102810   
         1048       0.02559       0.02831        0.03348      0.07678  0.025365   
         1110       0.01031       0.01338        0.02026      0.03092  0.011904   
         1097       0.01347       0.01686        0.02564      0.04040  0.029873   
         1198       0.01388       0.01709        0.02775      0.04163  0.028974   
         1211       0.01810       0.02261        0.03805      0.05429  0.038226   
         1223       0.01242       0.01705        0.02348      0.03727  0.025386   
         1350       0.00704       0.01018        0.01999      0.02112  0.014300   
         1436       0.01390       0.01619        0.02573      0.04171  0.025287   
         1472       0.01132       0.01379        0.01875      0.03396  0.037085   
         {\ldots}            {\ldots}           {\ldots}            {\ldots}          {\ldots}       {\ldots}   
         4396       0.03205       0.03165        0.03598      0.09616  0.034412   
         4407       0.03306       0.03670        0.04805      0.09918  0.051121   
         4418       0.01584       0.01645        0.02090      0.04751  0.022068   
         4531       0.01043       0.01063        0.01850      0.03128  0.031377   
         4570       0.00431       0.00653        0.01554      0.01294  0.013210   
         4610       0.00482       0.00675        0.01272      0.01446  0.006088   
         4692       0.01251       0.01570        0.02184      0.03754  0.011883   
         4706       0.03390       0.04308        0.06669      0.10170  0.033357   
         4718       0.02229       0.02932        0.05671      0.06687  0.013830   
         4857       0.10705       0.14281        0.13682      0.32114  0.554280   
         4930       0.08855       0.12206        0.10228      0.26564  0.413640   
         4878       0.08615       0.11476        0.12074      0.25844  0.448330   
         4986       0.02407       0.03832        0.06061      0.07222  0.036136   
         5044       0.02133       0.02329        0.02908      0.06398  0.011669   
         5009       0.01670       0.01657        0.01848      0.05011  0.026152   
         5126       0.01874       0.01735        0.02458      0.05622  0.014192   
         5213       0.02065       0.02300        0.02860      0.06194  0.034578   
         5150       0.01545       0.01702        0.02208      0.04634  0.033359   
         5275       0.01248       0.01441        0.02072      0.03744  0.025656   
         5405       0.00865       0.01007        0.01262      0.02594  0.023318   
         5417       0.01269       0.01421        0.01843      0.03806  0.030117   
         5418       0.02335       0.02953        0.03364      0.07006  0.064790   
         5429       0.01967       0.02066        0.02553      0.05902  0.020678   
         5440       0.02998       0.03217        0.04514      0.08994  0.037488   
         5560       0.01604       0.01922        0.02935      0.04811  0.047853   
         5683       0.01689       0.01721        0.02699      0.05068  0.030948   
         5576       0.01263       0.01443        0.02370      0.03790  0.020667   
         5725       0.05904       0.05315        0.04576      0.17713  0.132810   
         5762       0.01544       0.01757        0.02281      0.04633  0.026650   
         5749       0.01181       0.01623        0.02292      0.03543  0.016969   
         
                  HNR     RPDE      DFA      PPE  
         0     21.640  0.41888  0.54842  0.16006  
         12    22.203  0.56600  0.60571  0.20984  
         23    24.126  0.43806  0.59755  0.20164  
         149   20.632  0.54100  0.75905  0.19288  
         235   17.017  0.54534  0.76224  0.20882  
         173   21.598  0.52999  0.74918  0.21367  
         294   24.756  0.34321  0.56409  0.16791  
         306   26.742  0.48993  0.58972  0.16835  
         317   29.682  0.40309  0.55386  0.10145  
         438   20.493  0.70390  0.57545  0.18837  
         517   21.404  0.65353  0.61784  0.13389  
         460   18.783  0.67497  0.63997  0.26188  
         600   18.811  0.55403  0.66015  0.24177  
         579   21.921  0.59769  0.60524  0.18497  
         591   19.367  0.58626  0.67434  0.26485  
         731   15.320  0.65252  0.66396  0.26005  
         744   21.677  0.60817  0.64056  0.19316  
         834   17.306  0.56645  0.68765  0.28582  
         906   17.133  0.59826  0.64693  0.35412  
         973   18.938  0.61711  0.58763  0.22491  
         905   15.252  0.72632  0.60198  0.48600  
         1048  19.363  0.64008  0.70365  0.20803  
         1110  23.973  0.49407  0.61417  0.16862  
         1097  21.828  0.57525  0.65058  0.17626  
         1198  19.347  0.69439  0.75761  0.32504  
         1211  16.160  0.66448  0.75856  0.39718  
         1223  17.743  0.70789  0.80284  0.41966  
         1350  22.973  0.59941  0.63776  0.19357  
         1436  21.959  0.54587  0.66481  0.24183  
         1472  24.186  0.53256  0.61363  0.17815  
         {\ldots}      {\ldots}      {\ldots}      {\ldots}      {\ldots}  
         4396  15.742  0.60897  0.64520  0.25559  
         4407  14.314  0.61964  0.56749  0.28977  
         4418  17.916  0.57829  0.60455  0.25933  
         4531  19.613  0.66328  0.64633  0.18148  
         4570  20.851  0.65072  0.75381  0.20788  
         4610  24.008  0.50506  0.77542  0.17445  
         4692  21.893  0.46061  0.73700  0.27803  
         4706  16.475  0.55570  0.73365  0.37545  
         4718  21.323  0.54247  0.72771  0.25601  
         4857   2.964  0.75944  0.68087  0.37752  
         4930   4.638  0.71547  0.60625  0.38564  
         4878   4.673  0.72169  0.59709  0.64235  
         4986  19.083  0.63277  0.64222  0.14824  
         5044  21.650  0.49979  0.63568  0.17663  
         5009  19.537  0.46873  0.59211  0.16516  
         5126  22.450  0.52692  0.61574  0.14545  
         5213  18.816  0.67721  0.66215  0.33787  
         5150  18.198  0.69807  0.62595  0.33388  
         5275  24.515  0.56319  0.61741  0.17651  
         5405  25.093  0.53077  0.59072  0.17148  
         5417  23.651  0.61226  0.56940  0.19581  
         5418  15.274  0.63578  0.68440  0.34514  
         5429  21.321  0.53320  0.61901  0.17386  
         5440  17.094  0.64293  0.63920  0.22978  
         5560  18.812  0.55675  0.67940  0.33355  
         5683  19.954  0.56643  0.65006  0.22420  
         5576  22.864  0.53922  0.72680  0.31981  
         5725  11.689  0.70273  0.66975  0.48466  
         5762  22.184  0.60693  0.61840  0.19088  
         5749  21.992  0.63316  0.61075  0.19263  
         
         [126 rows x 22 columns]
\end{Verbatim}
            
    \subsubsection{Total UPDRS Score Over time since recruitment in the
study for the
subjects}\label{total-updrs-score-over-time-since-recruitment-in-the-study-for-the-subjects}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{fig2}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{n}{figsize}\PY{p}{,} \PY{n}{constrained\PYZus{}layout}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{axs} \PY{o}{=} \PY{n}{trim\PYZus{}axs}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{l+m+mi}{42}\PY{p}{)}
         \PY{k}{for} \PY{n}{ax}\PY{p}{,} \PY{n}{subject\PYZus{}id} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id =}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{str}\PY{p}{(}\PY{n}{subject\PYZus{}id}\PY{p}{)}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}xscale(\PYZsq{}log\PYZsq{})}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}yscale(\PYZsq{}log\PYZsq{})}
             \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{test\PYZus{}time}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}\PY{p}{)}\PY{p}{)}
         \PY{n}{fig2}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}updrs\PYZus{}time\PYZus{}test.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_42_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    TODO Marie: Regarder si df\_filtered\_3 fonctionne

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{nb\PYZus{}subjects\PYZus{}worsen} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{nb\PYZus{}subjects\PYZus{}improved} \PY{o}{=} \PY{l+m+mi}{0} 
        \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well} \PY{o}{=} \PY{l+m+mi}{0} 
        
        \PY{c+c1}{\PYZsh{} Dataframe containing only 3 UPDRS measures per patient. This will be useful to plot graphs }
        \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Go through all the 42 subject\PYZus{}id of the dataset }
        \PY{k}{for} \PY{n}{subject\PYZus{}id} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}     print(\PYZsq{}Subject\PYZus{}id : \PYZsq{}, subject\PYZus{}id)}
            \PY{c+c1}{\PYZsh{} Filter the dataframe to only the recordings of the current subject }
            \PY{n}{subject\PYZus{}df} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{)}\PY{p}{]}
        
            \PY{c+c1}{\PYZsh{} Then we will filter another time the subject\PYZus{}df because we want the value of the motor UPDRS}
            \PY{c+c1}{\PYZsh{} when test\PYZus{}time is at its minimum, median and max value}
            \PY{n}{min\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{min\PYZus{}subject\PYZus{}df}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}display(min\PYZus{}subject\PYZus{}df)}
            \PY{n}{min\PYZus{}value} \PY{o}{=} \PY{n}{min\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Min : \PYZsq{}, min\PYZus{}value)}
        
            \PY{c+c1}{\PYZsh{} Median uses row nearest to midpoint because otherwise median is a problem on dataframes of even length}
            \PY{n}{median\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{quantile}\PY{p}{(}\PY{n}{interpolation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nearest}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
            \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{median\PYZus{}subject\PYZus{}df}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}display(median\PYZus{}subject\PYZus{}df)}
            \PY{n}{median\PYZus{}value} \PY{o}{=} \PY{n}{median\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Median : \PYZsq{}, median\PYZus{}value)}
            
            \PY{n}{max\PYZus{}subject\PYZus{}df} \PY{o}{=} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{max\PYZus{}subject\PYZus{}df}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}display(max\PYZus{}subject\PYZus{}df)}
            \PY{n}{max\PYZus{}value} \PY{o}{=} \PY{n}{max\PYZus{}subject\PYZus{}df}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}\PY{o}{.}\PY{n}{values}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}print(\PYZsq{}Max : \PYZsq{}, max\PYZus{}value)}
            
            \PY{k}{if}\PY{p}{(}\PY{n}{min\PYZus{}value} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}worsen} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{elif}\PY{p}{(}\PY{n}{max\PYZus{}value} \PY{o}{\PYZlt{}} \PY{n}{min\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}improved} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{if}\PY{p}{(}\PY{n}{median\PYZus{}value} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}value}\PY{p}{)}\PY{p}{:}
                \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who got worst between start and the end of the trial: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}worsen}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who are even better than when they started the trial : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}improved}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Nb subjects who improved between midpoint and end : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{nb\PYZus{}subjects\PYZus{}midpoint\PYZus{}worst\PYZus{}end\PYZus{}well}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Nb subjects who got worst between start and the end of the trial:  29
Nb subjects who are even better than when they started the trial :  13
Nb subjects who improved between midpoint and end :  16

    \end{Verbatim}

    \subsubsection{Filtered 3 data points per
subject}\label{filtered-3-data-points-per-subject}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:}       subject\_id  age  sex  test\_time  motor\_UPDRS  total\_UPDRS  Jitter(\%)  \textbackslash{}
        0              1   72    0     5.6431       28.199       34.398    0.00662   
        24             1   72    0     5.6431       28.199       34.398    0.00348   
        12             1   72    0    89.6350       31.309       40.412    0.00524   
        36             1   72    0    89.6350       31.309       40.412    0.00991   
        61             1   72    0    89.6350       31.309       40.412    0.00423   
        23             1   72    0   174.6600       36.977       47.970    0.00386   
        48             1   72    0   174.6600       36.977       47.970    0.00369   
        73             1   72    0   174.6600       36.977       47.970    0.00508   
        98             1   72    0   174.6600       36.978       47.970    0.00547   
        123            1   72    0   174.6600       36.978       47.970    0.00567   
        148            1   72    0   174.6600       36.978       47.970    0.00526   
        149            2   58    0     3.8660       11.078       14.039    0.00600   
        174            2   58    0     3.8660       11.078       14.039    0.00606   
        198            2   58    0     3.8660       11.078       14.039    0.00725   
        235            2   58    0    90.8010       12.834       14.917    0.00524   
        259            2   58    0    90.8010       12.834       14.917    0.00611   
        283            2   58    0    90.8010       12.834       14.917    0.01032   
        173            2   58    0   178.8000       18.819       21.650    0.00440   
        197            2   58    0   178.8000       18.819       21.650    0.00437   
        222            2   58    0   178.8000       18.819       21.650    0.00317   
        247            2   58    0   178.8000       18.819       21.650    0.00417   
        271            2   58    0   178.8000       18.819       21.650    0.00482   
        293            2   58    0   178.8000       18.819       21.650    0.00650   
        294            3   57    0     7.2882       23.437       25.729    0.00565   
        318            3   57    0     7.2882       23.437       25.729    0.00695   
        342            3   57    0     7.2882       23.437       25.729    0.00527   
        306            3   57    0    91.3020       28.478       34.130    0.00293   
        317            3   57    0   168.2700       28.011       39.947    0.00199   
        341            3   57    0   168.2700       28.011       39.947    0.00213   
        365            3   57    0   168.2700       28.011       39.947    0.00355   
        {\ldots}          {\ldots}  {\ldots}  {\ldots}        {\ldots}          {\ldots}          {\ldots}        {\ldots}   
        5417          39   66    0   160.7000       34.064       44.298    0.00471   
        5418          40   85    1     7.7368       15.255       26.170    0.01258   
        5429          40   85    1    98.7420       17.745       27.490    0.00575   
        5452          40   85    1    98.7420       17.745       27.490    0.00637   
        5476          40   85    1    98.7420       17.745       27.490    0.00655   
        5440          40   85    1   176.7100       15.174       22.349    0.00561   
        5463          40   85    1   176.7100       15.174       22.349    0.00499   
        5487          40   85    1   176.7100       15.174       22.349    0.00581   
        5511          40   85    1   176.7100       15.174       22.349    0.01372   
        5535          40   85    1   176.7100       15.174       22.349    0.00857   
        5559          40   85    1   176.7100       15.174       22.349    0.00557   
        5560          41   68    1     2.4861       29.211       37.211    0.00835   
        5683          41   68    1    91.5260       36.771       44.771    0.00726   
        5711          41   68    1    91.5260       36.771       44.771    0.00636   
        5576          41   68    1   189.6500       33.643       42.515    0.00723   
        5603          41   68    1   189.6500       33.643       42.515    0.00795   
        5631          41   68    1   189.6500       33.643       42.515    0.00751   
        5658          41   68    1   189.6500       33.643       42.514    0.01286   
        5686          41   68    1   189.6500       33.643       42.514    0.00442   
        5714          41   68    1   189.6500       33.643       42.514    0.00415   
        5725          42   61    0    -4.2625       19.725       28.633    0.01533   
        5750          42   61    0    -4.2625       19.725       28.633    0.00615   
        5762          42   61    0    86.7650       25.598       36.464    0.00561   
        5787          42   61    0    86.7650       25.598       36.464    0.00621   
        5749          42   61    0   170.7300       20.513       31.513    0.00436   
        5774          42   61    0   170.7300       20.513       31.513    0.00718   
        5799          42   61    0   170.7300       20.513       31.513    0.00372   
        5824          42   61    0   170.7300       20.513       31.513    0.00920   
        5849          42   61    0   170.7300       20.513       31.513    0.00269   
        5874          42   61    0   170.7300       20.513       31.513    0.00282   
        
              Jitter(Abs)  Jitter:RAP  Jitter:PPQ5   {\ldots}     Shimmer(dB)  \textbackslash{}
        0        0.000034     0.00401      0.00317   {\ldots}           0.230   
        24       0.000015     0.00124      0.00133   {\ldots}           0.113   
        12       0.000033     0.00235      0.00251   {\ldots}           0.240   
        36       0.000057     0.00417      0.00546   {\ldots}           0.440   
        61       0.000023     0.00159      0.00180   {\ldots}           0.162   
        23       0.000023     0.00178      0.00195   {\ldots}           0.295   
        48       0.000022     0.00149      0.00183   {\ldots}           0.222   
        73       0.000027     0.00228      0.00258   {\ldots}           0.290   
        98       0.000024     0.00234      0.00268   {\ldots}           0.267   
        123      0.000024     0.00295      0.00254   {\ldots}           0.268   
        148      0.000026     0.00225      0.00177   {\ldots}           0.304   
        149      0.000048     0.00302      0.00294   {\ldots}           0.331   
        174      0.000047     0.00279      0.00311   {\ldots}           0.317   
        198      0.000055     0.00340      0.00330   {\ldots}           0.318   
        235      0.000041     0.00276      0.00296   {\ldots}           0.351   
        259      0.000042     0.00347      0.00352   {\ldots}           0.356   
        283      0.000074     0.00530      0.00489   {\ldots}           0.442   
        173      0.000033     0.00230      0.00282   {\ldots}           0.437   
        197      0.000033     0.00188      0.00224   {\ldots}           0.195   
        222      0.000023     0.00174      0.00187   {\ldots}           0.159   
        247      0.000031     0.00235      0.00232   {\ldots}           0.229   
        271      0.000033     0.00234      0.00211   {\ldots}           0.225   
        293      0.000049     0.00351      0.00355   {\ldots}           0.235   
        294      0.000031     0.00321      0.00325   {\ldots}           0.246   
        318      0.000039     0.00389      0.00361   {\ldots}           0.213   
        342      0.000029     0.00292      0.00324   {\ldots}           0.230   
        306      0.000023     0.00135      0.00178   {\ldots}           0.118   
        317      0.000013     0.00087      0.00115   {\ldots}           0.109   
        341      0.000014     0.00088      0.00140   {\ldots}           0.108   
        365      0.000023     0.00135      0.00119   {\ldots}           0.127   
        {\ldots}           {\ldots}         {\ldots}          {\ldots}   {\ldots}             {\ldots}   
        5417     0.000030     0.00193      0.00234   {\ldots}           0.238   
        5418     0.000081     0.00762      0.00721   {\ldots}           0.398   
        5429     0.000038     0.00319      0.00269   {\ldots}           0.317   
        5452     0.000045     0.00362      0.00344   {\ldots}           0.222   
        5476     0.000045     0.00335      0.00280   {\ldots}           0.224   
        5440     0.000059     0.00196      0.00291   {\ldots}           0.485   
        5463     0.000060     0.00164      0.00248   {\ldots}           0.326   
        5487     0.000048     0.00269      0.00273   {\ldots}           0.328   
        5511     0.000086     0.00752      0.00731   {\ldots}           0.459   
        5535     0.000060     0.00437      0.00362   {\ldots}           0.435   
        5559     0.000046     0.00172      0.00232   {\ldots}           0.380   
        5560     0.000043     0.00444      0.00431   {\ldots}           0.290   
        5683     0.000041     0.00351      0.00325   {\ldots}           0.300   
        5711     0.000034     0.00316      0.00332   {\ldots}           0.363   
        5576     0.000039     0.00317      0.00326   {\ldots}           0.235   
        5603     0.000043     0.00390      0.00400   {\ldots}           0.210   
        5631     0.000041     0.00380      0.00291   {\ldots}           0.181   
        5658     0.000069     0.00781      0.00668   {\ldots}           0.345   
        5686     0.000020     0.00196      0.00201   {\ldots}           0.143   
        5714     0.000018     0.00160      0.00210   {\ldots}           0.144   
        5725     0.000118     0.00774      0.00800   {\ldots}           0.908   
        5750     0.000046     0.00266      0.00291   {\ldots}           0.316   
        5762     0.000048     0.00232      0.00192   {\ldots}           0.278   
        5787     0.000052     0.00279      0.00325   {\ldots}           0.296   
        5749     0.000037     0.00181      0.00196   {\ldots}           0.235   
        5774     0.000059     0.00394      0.00341   {\ldots}           0.231   
        5799     0.000032     0.00180      0.00185   {\ldots}           0.146   
        5824     0.000073     0.00562      0.00568   {\ldots}           0.307   
        5849     0.000021     0.00121      0.00132   {\ldots}           0.154   
        5874     0.000021     0.00135      0.00166   {\ldots}           0.171   
        
              Shimmer:APQ3  Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR  \textbackslash{}
        0          0.01438       0.01309        0.01662      0.04314  0.014290   
        24         0.00411       0.00463        0.00949      0.01234  0.009238   
        12         0.01176       0.01395        0.02019      0.03528  0.011876   
        36         0.01934       0.02627        0.03872      0.05801  0.069650   
        61         0.00623       0.00658        0.01093      0.01868  0.041199   
        23         0.01312       0.01514        0.02626      0.03936  0.015298   
        48         0.01123       0.01306        0.02232      0.03370  0.010868   
        73         0.01390       0.01312        0.01911      0.04170  0.026755   
        98         0.00912       0.00969        0.01333      0.02735  0.017361   
        123        0.01469       0.01323        0.01801      0.04407  0.027539   
        148        0.01906       0.01497        0.02093      0.05718  0.033822   
        149        0.02058       0.02464        0.02778      0.06175  0.019901   
        174        0.01812       0.02032        0.02431      0.05436  0.020393   
        198        0.01908       0.02029        0.02445      0.05723  0.036085   
        235        0.02406       0.02581        0.03068      0.07217  0.022466   
        259        0.02096       0.02568        0.03591      0.06288  0.011063   
        283        0.02614       0.02640        0.02905      0.07843  0.032856   
        173        0.02352       0.03349        0.03659      0.07056  0.014770   
        197        0.01112       0.01269        0.01752      0.03335  0.008146   
        222        0.00939       0.01054        0.01387      0.02818  0.003074   
        247        0.01469       0.01560        0.01863      0.04406  0.006204   
        271        0.01174       0.01313        0.01788      0.03522  0.022158   
        293        0.01247       0.01429        0.01748      0.03741  0.011414   
        294        0.01442       0.01341        0.01500      0.04326  0.029994   
        318        0.01349       0.01431        0.01684      0.04048  0.018448   
        342        0.01465       0.01629        0.01951      0.04396  0.012843   
        306        0.00631       0.00788        0.01125      0.01892  0.006619   
        317        0.00573       0.00707        0.00952      0.01720  0.004082   
        341        0.00607       0.00790        0.01076      0.01822  0.002507   
        365        0.00678       0.00850        0.01120      0.02033  0.018264   
        {\ldots}            {\ldots}           {\ldots}            {\ldots}          {\ldots}       {\ldots}   
        5417       0.01269       0.01421        0.01843      0.03806  0.030117   
        5418       0.02335       0.02953        0.03364      0.07006  0.064790   
        5429       0.01967       0.02066        0.02553      0.05902  0.020678   
        5452       0.01282       0.01519        0.01888      0.03847  0.050715   
        5476       0.01305       0.01452        0.01912      0.03916  0.020820   
        5440       0.02998       0.03217        0.04514      0.08994  0.037488   
        5463       0.01786       0.02369        0.03124      0.05359  0.036949   
        5487       0.01798       0.02144        0.03522      0.05393  0.036552   
        5511       0.02886       0.03227        0.03513      0.08659  0.096704   
        5535       0.02939       0.02737        0.03468      0.08817  0.028398   
        5559       0.02031       0.02285        0.03199      0.06092  0.024145   
        5560       0.01604       0.01922        0.02935      0.04811  0.047853   
        5683       0.01689       0.01721        0.02699      0.05068  0.030948   
        5711       0.01515       0.01751        0.03408      0.04545  0.041365   
        5576       0.01263       0.01443        0.02370      0.03790  0.020667   
        5603       0.01028       0.01129        0.01890      0.03083  0.026547   
        5631       0.00925       0.00937        0.01581      0.02774  0.044729   
        5658       0.02251       0.02038        0.02790      0.06752  0.037285   
        5686       0.00694       0.00867        0.01735      0.02082  0.011369   
        5714       0.00754       0.00894        0.01433      0.02262  0.016191   
        5725       0.05904       0.05315        0.04576      0.17713  0.132810   
        5750       0.01907       0.02229        0.03109      0.05721  0.014472   
        5762       0.01544       0.01757        0.02281      0.04633  0.026650   
        5787       0.01052       0.01483        0.02574      0.03156  0.027826   
        5749       0.01181       0.01623        0.02292      0.03543  0.016969   
        5774       0.01176       0.01158        0.01653      0.03528  0.050160   
        5799       0.00693       0.00928        0.01568      0.02080  0.008305   
        5824       0.00944       0.01114        0.01722      0.02831  0.046031   
        5849       0.00945       0.00968        0.01353      0.02836  0.009154   
        5874       0.00946       0.01154        0.01470      0.02839  0.008172   
        
                 HNR     RPDE      DFA      PPE  
        0     21.640  0.41888  0.54842  0.16006  
        24    27.927  0.37340  0.52499  0.17066  
        12    22.203  0.56600  0.60571  0.20984  
        36    18.719  0.55590  0.59191  0.39101  
        61    24.186  0.48465  0.57380  0.18222  
        23    24.126  0.43806  0.59755  0.20164  
        48    25.000  0.49219  0.61602  0.18939  
        73    24.348  0.53511  0.59545  0.18327  
        98    24.543  0.52026  0.53879  0.29135  
        123   25.451  0.44728  0.52472  0.22185  
        148   25.852  0.45109  0.54152  0.23613  
        149   20.632  0.54100  0.75905  0.19288  
        174   22.218  0.55257  0.76217  0.32625  
        198   20.883  0.50137  0.75108  0.24236  
        235   17.017  0.54534  0.76224  0.20882  
        259   20.027  0.57732  0.78513  0.24013  
        283   17.792  0.54399  0.75126  0.20732  
        173   21.598  0.52999  0.74918  0.21367  
        197   25.919  0.44509  0.75375  0.18151  
        222   26.773  0.39745  0.72234  0.13860  
        247   23.445  0.46536  0.74583  0.14607  
        271   25.531  0.49237  0.74557  0.23923  
        293   23.168  0.45536  0.74959  0.21510  
        294   24.756  0.34321  0.56409  0.16791  
        318   19.743  0.46319  0.56117  0.23611  
        342   21.320  0.40321  0.56096  0.22595  
        306   26.742  0.48993  0.58972  0.16835  
        317   29.682  0.40309  0.55386  0.10145  
        341   27.710  0.44517  0.55230  0.15361  
        365   27.733  0.50847  0.55319  0.16096  
        {\ldots}      {\ldots}      {\ldots}      {\ldots}      {\ldots}  
        5417  23.651  0.61226  0.56940  0.19581  
        5418  15.274  0.63578  0.68440  0.34514  
        5429  21.321  0.53320  0.61901  0.17386  
        5452  18.618  0.60664  0.64009  0.25252  
        5476  23.162  0.50514  0.63544  0.17062  
        5440  17.094  0.64293  0.63920  0.22978  
        5463  19.372  0.70799  0.62080  0.21023  
        5487  19.785  0.65479  0.61777  0.22836  
        5511  16.619  0.61594  0.64678  0.32966  
        5535  19.523  0.58934  0.58872  0.18788  
        5559  20.546  0.50147  0.58653  0.13144  
        5560  18.812  0.55675  0.67940  0.33355  
        5683  19.954  0.56643  0.65006  0.22420  
        5711  17.798  0.65999  0.64159  0.32376  
        5576  22.864  0.53922  0.72680  0.31981  
        5603  24.731  0.55862  0.71723  0.35051  
        5631  23.764  0.65889  0.72497  0.28286  
        5658  18.482  0.56184  0.71760  0.26926  
        5686  24.882  0.56906  0.66522  0.23644  
        5714  25.069  0.51390  0.65669  0.24303  
        5725  11.689  0.70273  0.66975  0.48466  
        5750  21.127  0.49411  0.70068  0.22985  
        5762  22.184  0.60693  0.61840  0.19088  
        5787  22.292  0.52483  0.61083  0.13634  
        5749  21.992  0.63316  0.61075  0.19263  
        5774  23.357  0.44855  0.60768  0.25797  
        5799  24.747  0.41113  0.63415  0.15348  
        5824  21.821  0.44422  0.62241  0.30505  
        5849  25.280  0.49084  0.58159  0.12055  
        5874  23.259  0.58608  0.57077  0.15336  
        
        [403 rows x 22 columns]
\end{Verbatim}
            
    \subsubsection{Combien de recordings par
patient?}\label{combien-de-recordings-par-patient}

    Le nombre d'enregistrements que l'on possède par patient peut être très
déterminant. En effet, l'ensemble de données sur lequel on travaille est
dans le domaine de la santé, alors les résultats sont très en relation
avec le patient.

C'est pour cette raison que nous nous sommes demandés combien
d'enregistrements avons-nous pour chaque patient? La prochaine cellule
contient de l'information à ce sujet.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{patient\PYZus{}frequency} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{subject\PYZus{}id}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}
         \PY{n}{df\PYZus{}patient\PYZus{}frequency} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{patient\PYZus{}frequency}\PY{o}{.}\PY{n}{index}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{patient\PYZus{}frequency}\PY{o}{.}\PY{n}{values}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{df\PYZus{}patient\PYZus{}frequency}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:}     subject\_id  count
         0           29    168
         1           41    165
         2           35    165
         3            7    161
         4           34    161
         5            6    156
         6            5    156
         7           24    156
         8            9    152
         9           42    150
         10           8    150
         11          38    149
         12           1    149
         13          10    148
         14           2    145
         15          17    144
         16          25    144
         17           3    144
         18          39    143
         19          15    143
         20          40    142
         21          37    140
         22          11    138
         23          16    138
         24          23    138
         25           4    137
         26          14    136
         27          33    135
         28          28    134
         29          20    134
         30          26    130
         31          31    130
         32          36    129
         33          27    129
         34          19    129
         35          30    126
         36          18    126
         37          21    123
         38          13    112
         39          22    112
         40          12    107
         41          32    101
\end{Verbatim}
            
    Grâce à \texttt{describe()} encore une fois, on peut analyser justement
le nombre d'enregistrements par patients. En moyenne, les patients on
139 enregistrement. Le minimum d'enregistrements qu'un patient possède
est 101, ce qui est une bonne nouvelle, puisque le nombre
d'enregistrments ne varie pas trop d'un patient à l'autre. Le maximum
d'enregistrements pour un patient est de 168.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{df}\PY{o}{.}\PY{n}{subject\PYZus{}id}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} count     42.000000
         mean     139.880952
         std       15.542550
         min      101.000000
         25\%      130.000000
         50\%      141.000000
         75\%      149.750000
         max      168.000000
         Name: subject\_id, dtype: float64
\end{Verbatim}
            
    \subsection{Objective 2: Data analysis}\label{objective-2-data-analysis}

    \subsubsection{Is there a correlation between the age of the patients
and their motor UPDRS score or total
score?}\label{is-there-a-correlation-between-the-age-of-the-patients-and-their-motor-updrs-score-or-total-score}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}\PY{p}{;} \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{p}{)}
         \PY{n}{dfage}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{as\PYZus{}index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{regplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{dfage}\PY{p}{,} \PY{n}{line\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{collections}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confidence interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean motor\PYZus{}UPDRS by age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{=}\PY{n}{sns}\PY{o}{.}\PY{n}{regplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{dfage}\PY{p}{,} \PY{n}{line\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Linear Regression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{collections}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}label}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Confidence interval}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean total\PYZus{}UPDRS by age}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Is there a correlation between the sex of the patient and
their motor UPDRS score or total
score?}\label{is-there-a-correlation-between-the-sex-of-the-patient-and-their-motor-updrs-score-or-total-score}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sex}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{as\PYZus{}index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:}    sex  subject\_id        age  test\_time  motor\_UPDRS  total\_UPDRS  Jitter(\%)  \textbackslash{}
         0    0   19.072106  65.055389  93.221349    21.469349    29.724055   0.005956   
         1    1   26.693626  64.267274  92.095983    20.924581    27.505234   0.006577   
         
            Jitter(Abs)  Jitter:RAP  Jitter:PPQ5    {\ldots}     Shimmer(dB)  Shimmer:APQ3  \textbackslash{}
         0     0.000048    0.002824     0.003053    {\ldots}        0.302085      0.016750   
         1     0.000036    0.003338     0.003758    {\ldots}        0.330013      0.018027   
         
            Shimmer:APQ5  Shimmer:APQ11  Shimmer:DDA       NHR        HNR      RPDE  \textbackslash{}
         0      0.019407       0.027162     0.050249  0.025269  21.679985  0.552449   
         1      0.021727       0.028165     0.054081  0.046827  21.678445  0.517910   
         
                 DFA       PPE  
         0  0.661229  0.225827  
         1  0.636088  0.206197  
         
         [2 rows x 22 columns]
\end{Verbatim}
            
    \subsubsection{Can we identify some clusters in the data? (Différence de
variabilité entre les
échantillons)}\label{can-we-identify-some-clusters-in-the-data-diffuxe9rence-de-variabilituxe9-entre-les-uxe9chantillons}

    Tu utilises le motor ou le total UPDRS ?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{}Moyenne des enregistrement des sujets}
         \PY{n}{Mean}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{as\PYZus{}index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Retirer les données non désirées}
         \PY{n}{X}\PY{o}{=}\PY{n}{Mean}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Application lDA}
         \PY{n}{X}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         \PY{n}{model\PYZus{}LSA} \PY{o}{=} \PY{n}{TruncatedSVD}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{U} \PY{o}{=} \PY{n}{model\PYZus{}LSA}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{V} \PY{o}{=} \PY{n}{model\PYZus{}LSA}\PY{o}{.}\PY{n}{components\PYZus{}}
         
         \PY{c+c1}{\PYZsh{}Représentation}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{,}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Subject illustration of 2d space by LSA factorisation}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{First ax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Decond ax}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{files\PYZus{}name} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{V}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{textcoords}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{offset points}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{xytext}\PY{o}{=}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{table}\PY{p}{(}\PY{n}{cellText}\PY{o}{=}\PY{n}{Mean}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,} 
                   \PY{n}{colLabels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_59_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{Data}\PY{o}{=}\PY{n}{V}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Application Clustering}
         \PY{n}{kmeans} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{n\PYZus{}clusters}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
         \PY{n}{kmeans}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{Data}\PY{p}{)}
         \PY{n}{y\PYZus{}kmeans} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{Data}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Représentation}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{Data}\PY{p}{[}\PY{p}{:} \PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{Data}\PY{p}{[}\PY{p}{:} \PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y\PYZus{}kmeans}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{25}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{viridis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{files\PYZus{}name} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{annotate}\PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{V}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{,} \PY{n}{V}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{textcoords}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{offset points}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{xytext}\PY{o}{=}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)} 
         \PY{n}{plt}\PY{o}{.}\PY{n}{table}\PY{p}{(}\PY{n}{cellText}\PY{o}{=}\PY{n}{Mean}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{p}{,} 
                   \PY{n}{colLabels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{centers} \PY{o}{=} \PY{n}{kmeans}\PY{o}{.}\PY{n}{cluster\PYZus{}centers\PYZus{}}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{centers}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{centers}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Objective 3: Modélisation}\label{objective-3-moduxe9lisation}

    Can we predict the motor UPDRS (or total) score from 16 vocal features?

    Source: *
https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998

Intéressant: *
https://stats.stackexchange.com/questions/393330/sklearn-support-vector-regression-test-data-prediction-is-constant

    \subsubsection{Splitting the data}\label{splitting-the-data}

Splitting the data in 80/20. 80\% of the data will be for
training/development and the remaining 20\% will be for test.

Some questions remains though: * Is it better to split while making sure
train/dev/test all have data associated to all subjects? * What if we
test on the data of a subject we have never seen before?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} prepare dataset with input and output scalers, can be none}
        \PY{k+kn}{import} \PY{n+nn}{tensorflow} \PY{k}{as} \PY{n+nn}{tf}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{models} \PY{k}{import} \PY{n}{Sequential}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{layers} \PY{k}{import} \PY{n}{Dense}
        \PY{k+kn}{from} \PY{n+nn}{keras}\PY{n+nn}{.}\PY{n+nn}{optimizers} \PY{k}{import} \PY{n}{SGD}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{OneHotEncoder}
        \PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{mean}
        \PY{k+kn}{from} \PY{n+nn}{numpy} \PY{k}{import} \PY{n}{std}
        
        \PY{k}{def} \PY{n+nf}{get\PYZus{}dataset}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{}     X = df[df.columns.difference([\PYZsq{}motor\PYZus{}UPDRS\PYZsq{}, \PYZsq{}total\PYZus{}UPDRS\PYZsq{}, \PYZsq{}subject\PYZus{}id\PYZsq{}])]}
            \PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{difference}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}
        \PY{c+c1}{\PYZsh{}     enc = OneHotEncoder(handle\PYZus{}unknown=\PYZsq{}ignore\PYZsq{})}
        \PY{c+c1}{\PYZsh{}     enc.fit(X)}
            \PY{k}{if} \PY{n}{label} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
                \PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{total\PYZus{}UPDRS}
            
            \PY{c+c1}{\PYZsh{} Split into train and test set 80/20}
            \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}scale inputs }
            \PY{k}{if} \PY{n}{input\PYZus{}scaler} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} fit scaler}
                \PY{n}{input\PYZus{}scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} transform training dataset}
                \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{]}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} transform test dataset}
                \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{input\PYZus{}scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
            \PY{k}{if} \PY{n}{output\PYZus{}scaler} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} reshape 1d arrays to 2d arrays}
                \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} fit scaler on training dataset}
                \PY{n}{output\PYZus{}scaler}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} transform training dataset}
                \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{output\PYZus{}scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} transform test dataset}
                \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{output\PYZus{}scaler}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorflow\textbackslash{}python\textbackslash{}framework\textbackslash{}dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np\_resource = np.dtype([("resource", np.ubyte, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}h5py\textbackslash{}\_\_init\_\_.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems
  '\{0\}.\{1\}.\{2\}'.format(*version.hdf5\_built\_version\_tuple)
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint8 = np.dtype([("qint8", np.int8, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint8 = np.dtype([("quint8", np.uint8, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint16 = np.dtype([("qint16", np.int16, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_quint16 = np.dtype([("quint16", np.uint16, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  \_np\_qint32 = np.dtype([("qint32", np.int32, 1)])
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}tensorboard\textbackslash{}compat\textbackslash{}tensorflow\_stub\textbackslash{}dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np\_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.

    \end{Verbatim}

    \subsubsection{Gridsearch}\label{gridsearch}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} \PYZsq{}gamma\PYZsq{}: [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]}
         \PY{c+c1}{\PYZsh{} tuned\PYZus{}parameters = [\PYZob{}\PYZsq{}kernel\PYZsq{}: [\PYZsq{}rbf\PYZsq{}], \PYZsq{}gamma\PYZsq{}: [1e\PYZhy{}3, 1e\PYZhy{}4],}
         \PY{c+c1}{\PYZsh{}                      \PYZsq{}epsilon\PYZsq{}: [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],}
         \PY{c+c1}{\PYZsh{}                      \PYZsq{}C\PYZsq{}: [0.1, 1, 10, 100, 1000]\PYZcb{},}
         \PY{c+c1}{\PYZsh{}                     \PYZob{}\PYZsq{}kernel\PYZsq{}: [\PYZsq{}linear\PYZsq{}], \PYZsq{}C\PYZsq{}: [0.1, 1, 10, 100, 1000]\PYZcb{}]}
         
         
         \PY{c+c1}{\PYZsh{} Bigger Gridsearch}
         \PY{c+c1}{\PYZsh{} tuned\PYZus{}parameters = [\PYZob{}\PYZsq{}kernel\PYZsq{}: [\PYZsq{}rbf\PYZsq{}], \PYZsq{}gamma\PYZsq{}: [1e\PYZhy{}3, 1e\PYZhy{}4],}
         \PY{c+c1}{\PYZsh{}                      \PYZsq{}epsilon\PYZsq{}: [0.01, 0.1, 0.5, 1, 10],}
         \PY{c+c1}{\PYZsh{}                      \PYZsq{}C\PYZsq{}:  [0.1, 1, 10, 100]\PYZcb{},}
         \PY{c+c1}{\PYZsh{}                     \PYZob{}\PYZsq{}kernel\PYZsq{}: [\PYZsq{}linear\PYZsq{}], \PYZsq{}C\PYZsq{}: [0.1, 1, 10, 100]\PYZcb{}]}
         
         \PY{c+c1}{\PYZsh{} Gridsearch because I\PYZsq{}m hesitating between rbf or linear kernel }
         \PY{n}{tuned\PYZus{}parameters} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gamma}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{]}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epsilon}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}  \PY{p}{[}\PY{l+m+mi}{100}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
                             \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kernel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{]}
         
         \PY{k}{def} \PY{n+nf}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{=}\PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,}\PY{n}{output\PYZus{}scaler}\PY{p}{,}\PY{n}{label}\PY{p}{)}
             \PY{n}{gsc} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}
                     \PY{n}{estimator}\PY{o}{=}\PY{n}{SVR}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                     \PY{n}{param\PYZus{}grid}\PY{o}{=}\PY{n}{tuned\PYZus{}parameters}\PY{p}{,}
                     \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
                     \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         
             \PY{n}{grid\PYZus{}result} \PY{o}{=} \PY{n}{gsc}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{best\PYZus{}params} \PY{o}{=} \PY{n}{grid\PYZus{}result}\PY{o}{.}\PY{n}{best\PYZus{}params\PYZus{}}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} For input : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{)} \PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ output scaler : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{output\PYZus{}scaler}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ and label : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{n}{best\PYZus{}params}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Gridsearch Best Params for label Motor with different
pre-processing
options}\label{gridsearch-best-params-for-label-motor-with-different-pre-processing-options}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  None  output scaler :  None  and label :  motor
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{} For input :  None  output scaler :  None  and label :  motor}
        \PY{c+c1}{\PYZsh{} \PYZob{}\PYZsq{}C\PYZsq{}: 100, \PYZsq{}epsilon\PYZsq{}: 1, \PYZsq{}gamma\PYZsq{}: 0.001, \PYZsq{}kernel\PYZsq{}: \PYZsq{}rbf\PYZsq{}\PYZcb{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  None  output scaler :  None  and label :  motor
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  None  output scaler :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  and label :  motor
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  MinMaxScaler(copy=True, feature\_range=(0, 1))  output scaler :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  and label :  motor
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  output scaler :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  and label :  motor
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \subsubsection{Gridsearch Best Params for label total with different
pre-processing
options}\label{gridsearch-best-params-for-label-total-with-different-pre-processing-options}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,}\PY{k+kc}{None}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  None  output scaler :  None  and label :  total
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  None  output scaler :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  and label :  total
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)
/Users/marie-philippe/anaconda3/envs/gti770\_py3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n\_samples, ), for example using ravel().
  y = column\_or\_1d(y, warn=True)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
---- For input :  MinMaxScaler(copy=True, feature\_range=(0, 1))  output scaler :  StandardScaler(copy=True, with\_mean=True, with\_std=True)  and label :  total
\{'C': 100, 'epsilon': 1, 'gamma': 0.001, 'kernel': 'rbf'\}

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{gridsearch\PYZus{}svr}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \subsection{Going to see how much scaling can help improve
results}\label{going-to-see-how-much-scaling-can-help-improve-results}

    Reference:
https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python

Different scaling inspiration:
https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler}
         \PY{n}{sc} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)} \PY{c+c1}{\PYZsh{} normalizing the features}
         \PY{n}{X\PYZus{}test\PYZus{}scaled} \PY{o}{=} \PY{n}{sc}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Let}\PY{l+s+se}{\PYZbs{}\PYZsq{}}\PY{l+s+s1}{s verify the mean is around 0 and the standard deviation is around 1.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Mean : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scaled}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Std dev : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scaled}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} convert back to dataframe format }
         \PY{n}{X\PYZus{}train\PYZus{}scaled} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scaled}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Let's verify the mean is around 0 and the standard deviation is around 1.
Mean :  -9.116112120496605e-17
Std dev :  1.0

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
         
         \PY{k}{def} \PY{n+nf}{train\PYZus{}results}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{,} \PY{n}{label}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}train type : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}     best\PYZus{}svr = SVR(C=0.1, kernel=\PYZsq{}linear\PYZsq{})}
             \PY{n}{best\PYZus{}svr} \PY{o}{=} \PY{n}{SVR}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{epsilon}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{gamma}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{)}
             \PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
             \PY{n}{train\PYZus{}r2\PYZus{}score} \PY{o}{=} \PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training score : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train\PYZus{}r2\PYZus{}score}\PY{p}{)}
         
             \PY{n}{y\PYZus{}test\PYZus{}pred} \PY{o}{=} \PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
             \PY{n}{test\PYZus{}r2\PYZus{}score} \PY{o}{=} \PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Testing score : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{test\PYZus{}r2\PYZus{}score}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Testing MSE Score : }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{features\PYZus{}names} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}
             \PY{n}{features\PYZus{}names} \PY{o}{=} \PY{n}{features\PYZus{}names}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}     print(best\PYZus{}svr.coef\PYZus{})}
             \PY{n}{f\PYZus{}importances}\PY{p}{(}\PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{features\PYZus{}names}\PY{p}{)}
             
         \PY{c+c1}{\PYZsh{}     print(X\PYZus{}test.shape)}
         \PY{c+c1}{\PYZsh{}     print(y\PYZus{}test\PYZus{}pred.shape)}
         \PY{c+c1}{\PYZsh{}     print(y\PYZus{}test.shape)}
         \PY{c+c1}{\PYZsh{}     plt.scatter(X\PYZus{}test.Shimmer, y\PYZus{}test, s=5, color=\PYZdq{}blue\PYZdq{}, label=\PYZdq{}original\PYZdq{})}
         \PY{c+c1}{\PYZsh{}     plt.scatter(X\PYZus{}test.Shimmer, y\PYZus{}test\PYZus{}pred, lw=2, color=\PYZdq{}red\PYZdq{}, label=\PYZdq{}fitted\PYZdq{})}
         \PY{c+c1}{\PYZsh{}     plt.legend()}
         \PY{c+c1}{\PYZsh{}     plt.show()}
\end{Verbatim}


    TODO Marie: Pourquoi le MSE est vraiment haut quand unscaled?

    \subsubsection{Predicting total UPDRS}\label{predicting-total-updrs}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No input scaler, no output scaler}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} unscaled inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unscaled inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print(\PYZsq{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZsq{})}
         
         \PY{c+c1}{\PYZsh{} normalized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}normalized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print(\PYZsq{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZsq{})}
         
         \PY{c+c1}{\PYZsh{} standardized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{standardized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}standardized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
No input scaler, no output scaler
X\_train type :  <class 'pandas.core.frame.DataFrame'>
Training score :  0.17614389119137985
Testing score :  0.1830013453878776
Testing MSE Score :  98.70631831039448

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
---
Unscaled inputs
X\_train type :  <class 'pandas.core.frame.DataFrame'>
Training score :  0.20191260729538707
Testing score :  0.2078027754429823
Testing MSE Score :  0.8478204817011479

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
normalized inputs
X\_train type :  <class 'pandas.core.frame.DataFrame'>
Training score :  0.20855176783397744
Testing score :  0.2136419966084917
Testing MSE Score :  0.8415712660414112

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
standardized inputs
X\_train type :  <class 'pandas.core.frame.DataFrame'>
Training score :  0.22052387732004097
Testing score :  0.22762854999629886
Testing MSE Score :  0.8266026621849405

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_7.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No input scaler, no output scaler}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} unscaled inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unscaled inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} normalized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}normalized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} standardized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{standardized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}standardized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{total}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
No input scaler, no output scaler
Training score :  0.3784264350796862
Testing score :  0.365918630651497
Testing MSE Score :  76.60702636936202
---
Unscaled inputs
Training score :  0.4283640165308449
Testing score :  0.434151995664797
Testing MSE Score :  0.6055783998402235
---
normalized inputs
Training score :  0.20928960427564502
Testing score :  0.21526327034894444
Testing MSE Score :  0.8398361563477756
---
standardized inputs
Training score :  0.2652304771914429
Testing score :  0.2640927277773192
Testing MSE Score :  0.7875781922514236

    \end{Verbatim}

    \subsubsection{Predicting Motor UPDRS}\label{predicting-motor-updrs}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{No input scaler, no output scaler}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} unscaled inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unscaled inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} normalized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}normalized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} standardized inputs}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{standardized inputs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{results\PYZus{}standardized\PYZus{}inputs} \PY{o}{=} \PY{n}{train\PYZus{}results}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
No input scaler, no output scaler
Training score :  0.08598113019387277
Testing score :  0.07717743621670414
Testing MSE Score :  64.00922922273243
---
Unscaled inputs
Training score :  0.15105585858891935
Testing score :  0.1395675365964988
Testing MSE Score :  0.9147399692294598
---
normalized inputs
Training score :  0.10432514385378255
Testing score :  0.10852869972458257
Testing MSE Score :  0.9477378695792752
---
standardized inputs
Training score :  0.17056147855960246
Testing score :  0.15984799821918938
Testing MSE Score :  0.8931794753734771

    \end{Verbatim}

    TODO: Reste à interpréter les scores, dire pourquoi le unscaled inputs
performe mieux, expliquer c'est quoi chaque type de standardization,
est-ce que les SVR en bénéficie? features deviennent indépendantes?

    \subsubsection{Visualizing most important
features}\label{visualizing-most-important-features}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} Source: https://stackoverflow.com/questions/41592661/determining\PYZhy{}the\PYZhy{}most\PYZhy{}contributing\PYZhy{}features\PYZhy{}for\PYZhy{}svm\PYZhy{}classifier\PYZhy{}in\PYZhy{}sklearn}
         
         \PY{k+kn}{from} \PY{n+nn}{matplotlib} \PY{k}{import} \PY{n}{pyplot} \PY{k}{as} \PY{n}{plt}
         \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{svm}
         
         \PY{k}{def} \PY{n+nf}{f\PYZus{}importances}\PY{p}{(}\PY{n}{coef}\PY{p}{,} \PY{n}{names}\PY{p}{)}\PY{p}{:}
             \PY{n}{imp} \PY{o}{=} \PY{n}{coef}
             \PY{n}{imp}\PY{p}{,}\PY{n}{names} \PY{o}{=} \PY{n+nb}{zip}\PY{p}{(}\PY{o}{*}\PY{n+nb}{sorted}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{imp}\PY{p}{,}\PY{n}{names}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{barh}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{names}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{imp}\PY{p}{,} \PY{n}{align}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{yticks}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{names}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{names}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{features\PYZus{}names} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)} \PY{c+c1}{\PYZsh{}[\PYZsq{}input1\PYZsq{}, \PYZsq{}input2\PYZsq{}]}
         \PY{n}{best\PYZus{}svr} \PY{o}{=} \PY{n}{SVR}\PY{p}{(}\PY{n}{kernel}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}
         \PY{n}{best\PYZus{}svr}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        NameError                                 Traceback (most recent call last)

        <ipython-input-38-e9be1eacc552> in <module>()
         11     plt.show()
         12 
    ---> 13 features\_names = X\_train.keys() \#['input1', 'input2']
         14 best\_svr = SVR(kernel='linear', C=0.1)
         15 best\_svr.fit(X\_train, y\_train.ravel())
    

        NameError: name 'X\_train' is not defined

    \end{Verbatim}

    \subsection{PCA}\label{pca}

    \subsubsection{Create covariance matrix}\label{create-covariance-matrix}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
         
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{k+kc}{None}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{motor}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         
         \PY{n}{covar\PYZus{}matrix} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{whiten}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{covar\PYZus{}matrix}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{variance} \PY{o}{=} \PY{n}{covar\PYZus{}matrix}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}} \PY{c+c1}{\PYZsh{}calculate variance ratios}
         \PY{n}{var}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{cumsum}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{covar\PYZus{}matrix}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{,} \PY{n}{decimals}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
         \PY{n}{var} \PY{c+c1}{\PYZsh{}cumulative sum of variance explained with [n] features}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} array([ 91.9,  96.9,  99.4, 100. , 100. , 100. , 100. , 100. , 100. ,
                100. , 100. , 100. , 100. , 100. , 100. , 100. , 100. , 100. ,
                100. , 100. ])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{covar\PYZus{}matrix}\PY{o}{.}\PY{n}{components\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}29}]:} array([[ 3.23436756e-05,  3.06344041e-03, -2.38865258e-06,
                 -9.84744269e-09, -4.80235620e-06, -1.47095911e-06,
                 -1.60216756e-06, -3.45757071e-05, -5.21803276e-06,
                 -8.78041240e-05, -1.77064932e-05, -1.40971113e-04,
                 -1.53441071e-05, -7.94892317e-06, -1.19707550e-05,
                 -2.38450668e-05,  2.35363078e-03, -7.62530024e-05,
                  5.36188606e-05,  9.99992518e-01],
                [-5.19922434e-04,  7.81686486e-02, -6.50353642e-05,
                 -2.51280504e-07, -9.76642457e-05, -4.27223273e-05,
                 -3.25521843e-05, -8.95996885e-04, -1.23940576e-03,
                 -1.30499052e-03, -3.34191025e-04, -2.91853330e-03,
                 -2.96333697e-04, -1.33731603e-04, -1.99967969e-04,
                 -4.01191313e-04,  3.50380138e-02, -1.07900158e-02,
                 -9.96259122e-01, -2.69908926e-04],
                [ 6.81686549e-04,  7.29374911e-02, -3.79646605e-05,
                 -2.83861218e-07, -4.90929780e-05, -2.07135802e-05,
                 -1.63594405e-05, -2.95581419e-04, -1.62166955e-03,
                 -1.42347040e-03, -3.99713325e-04, -3.79997323e-03,
                 -3.84772232e-04, -1.99975607e-04, -2.31914254e-04,
                 -5.99927635e-04, -9.96890923e-01,  1.67680769e-03,
                 -2.93406402e-02,  2.12385702e-03],
                [ 4.73102735e-03, -9.92927728e-01,  8.79644831e-04,
                  6.05280228e-06,  1.40041673e-03,  5.67405299e-04,
                  4.66798636e-04,  9.64911755e-03,  1.57198928e-02,
                  1.52953027e-02,  4.85693434e-03,  4.35069316e-02,
                  3.54044712e-03,  2.43326348e-03,  3.08069679e-03,
                  7.29980571e-03, -7.05028613e-02, -5.85322786e-03,
                 -8.05092880e-02,  3.21972707e-03],
                [-3.01792738e-02, -4.20116075e-03,  6.11329722e-04,
                 -1.20517613e-05,  1.62657978e-03,  7.34499044e-04,
                  5.42401921e-04,  2.27958570e-02, -2.21897997e-02,
                 -4.10917723e-02,  4.41910810e-03,  3.89579210e-02,
                  1.34796936e-03,  2.06288339e-03,  3.08050528e-03,
                  6.18876440e-03,  1.60915883e-03,  9.97321326e-01,
                 -1.11175500e-02,  8.95715812e-05],
                [-9.76560163e-02,  4.48484198e-02,  1.12812539e-02,
                  4.12437601e-05,  1.81279276e-02,  8.50339675e-03,
                  6.04260430e-03,  1.77545535e-01,  2.74568398e-02,
                 -6.44001032e-02,  1.04412610e-01,  9.49552972e-01,
                  6.97979345e-02,  5.24247418e-02,  6.53099845e-02,
                  1.57274968e-01, -7.11830568e-04, -4.78353568e-02,
                  1.02938586e-03,  6.23244358e-06],
                [-1.17459900e-01,  1.92079071e-02,  5.04962692e-03,
                  6.56456015e-05,  4.78846651e-03,  1.40533130e-03,
                  1.59676496e-03,  7.08248508e-02,  1.62765372e-01,
                  9.75165675e-01,  1.00836172e-03,  3.97253803e-02,
                  5.19973151e-03, -5.68886601e-03, -2.52396220e-03,
                 -1.70698217e-02, -4.17922153e-04,  3.72524884e-02,
                 -5.04355435e-04,  4.29851111e-05],
                [ 8.13924542e-01,  1.56443725e-02,  1.73905668e-02,
                  1.46067917e-04,  2.67689950e-02,  8.14286276e-03,
                  8.92295169e-03, -6.82499897e-02,  5.68936892e-01,
                  2.82353636e-03,  5.04909366e-03,  7.85182604e-02,
                  1.68986331e-02,  1.27994185e-04,  6.20952102e-04,
                  3.85920097e-04,  5.54972098e-04,  3.58422920e-02,
                 -4.57476635e-04, -6.03115699e-05],
                [ 5.05827039e-01, -3.27850856e-03, -3.20614654e-02,
                 -1.71121946e-04, -4.88669904e-02, -1.94346873e-02,
                 -1.62896048e-02, -2.78108878e-01, -7.72688190e-01,
                  2.06631004e-01,  2.70700689e-02,  1.22587124e-01,
                  2.86517183e-02,  2.43642525e-02,  2.01424932e-02,
                  7.30945269e-02,  6.26057389e-04,  7.57880380e-03,
                 -4.57799291e-05,  1.72975038e-05],
                [ 2.38064834e-01,  5.25338683e-04,  6.03482430e-02,
                  2.84470249e-04,  1.00290269e-01,  4.49706640e-02,
                  3.34307494e-02,  9.22723371e-01, -2.18037194e-01,
                  1.20576131e-03, -7.26402832e-03, -1.24994758e-01,
                 -4.66526623e-02, -3.06980276e-02,  8.27193303e-03,
                 -9.20977117e-02,  8.08365904e-04, -1.33527286e-02,
                 -6.49226899e-05, -1.41527705e-06],
                [-4.68922934e-03,  8.38157802e-04,  7.12754817e-02,
                  5.39179885e-04,  1.82433158e-01,  3.26526039e-02,
                  6.08190833e-02,  3.50441594e-02,  2.89912724e-02,
                  1.54269092e-02,  1.51800462e-01, -1.62976075e-01,
                 -3.33932565e-01,  2.81152897e-01,  5.42898066e-02,
                  8.43465077e-01,  7.52182471e-05,  1.05578281e-04,
                  9.35414097e-05, -1.09437671e-06],
                [-2.86540390e-03,  6.77054702e-04, -1.54365744e-01,
                 -7.45700355e-04, -3.12514681e-01, -7.30147908e-02,
                 -1.04170228e-01,  1.05071746e-01,  3.62769443e-02,
                 -3.02875372e-03,  2.21563418e-01, -1.64454244e-01,
                  7.62838679e-01,  8.82097575e-02,  3.37765124e-01,
                  2.64620223e-01, -2.35892818e-05,  4.54536583e-04,
                 -4.52048155e-05,  3.02288936e-06],
                [-2.57831964e-02, -1.20136768e-04,  3.92645867e-01,
                  1.43400766e-03,  7.32828823e-01,  2.07412148e-01,
                  2.44282147e-01, -1.16527644e-01, -3.89944706e-02,
                  3.55953279e-03,  6.08553703e-02, -3.25771631e-02,
                  4.21961797e-01, -1.96391351e-02,  6.04076847e-02,
                 -5.88863190e-02, -3.04779880e-05,  9.24234183e-05,
                 -9.15028148e-09,  4.63709351e-06],
                [-5.21866559e-03, -2.44448148e-05,  6.48936462e-02,
                 -8.74121794e-04, -1.60077564e-02,  1.88563567e-01,
                 -5.32658459e-03, -5.89743601e-02,  5.30873437e-03,
                  1.98660252e-03,  3.41906450e-01, -1.71674059e-02,
                 -3.41609030e-01, -7.77770041e-02,  8.13665403e-01,
                 -2.33276119e-01,  2.15631158e-05, -4.70600224e-05,
                  1.36003249e-05,  6.50255985e-07],
                [-1.07917451e-03, -6.07581651e-05, -2.04289001e-02,
                  1.96975731e-04, -1.10623559e-02, -3.02206740e-02,
                 -3.71310108e-03, -6.55317099e-03,  1.63525894e-03,
                 -2.55247087e-03,  8.90913252e-01, -3.94265965e-02,
                 -3.78997497e-02, -4.48850854e-02, -4.26131139e-01,
                 -1.34759711e-01, -2.75370321e-06, -8.26403280e-06,
                 -1.94285634e-05,  5.97588799e-07],
                [-4.03714605e-04, -2.63053906e-05,  4.52095626e-01,
                 -2.23827345e-03, -4.12490381e-01,  7.55636544e-01,
                 -1.37568608e-01, -7.60645122e-03, -4.15232009e-03,
                  5.47034778e-04, -3.86986281e-02,  1.29202318e-04,
                  4.03338467e-02,  1.97828199e-02, -1.68615514e-01,
                  5.93458107e-02, -4.86609175e-06,  2.29127228e-04,
                 -2.30370860e-06, -6.18107642e-07],
                [-1.03467058e-04, -3.06355629e-05,  7.76370406e-01,
                  6.74281154e-03, -2.18987251e-01, -5.83571726e-01,
                 -7.29310899e-02,  1.04274909e-03, -2.87393655e-03,
                 -1.38646690e-03,  1.67753965e-02, -2.05496150e-03,
                 -2.36251543e-02, -1.92167023e-03,  4.98247343e-02,
                 -5.65069761e-03,  1.23473866e-06,  1.25631003e-04,
                 -1.06774643e-06, -7.78606358e-08],
                [ 5.58985047e-05, -2.57075706e-07,  4.90488879e-03,
                 -9.99970416e-01,  2.76957325e-04, -5.40974410e-03,
                  2.11177175e-03,  1.59628815e-04,  8.37430636e-05,
                  3.02130686e-05,  7.86862509e-05, -2.23596042e-05,
                 -1.14705984e-04, -9.81852752e-04, -2.16383247e-04,
                  4.89559794e-04, -1.06160645e-08, -1.12164806e-05,
                 -9.63851811e-08,  3.14819875e-09],
                [ 4.72634577e-07, -2.41300391e-08,  1.07847716e-06,
                  8.19648692e-04, -2.47194244e-01,  6.02396148e-05,
                  7.41530072e-01, -2.85296538e-06, -7.89839905e-07,
                 -9.08077585e-07, -3.77591588e-06,  1.63547481e-06,
                  1.84154270e-06,  5.91712679e-01, -7.19130599e-06,
                 -1.97239925e-01,  2.49952205e-09, -1.27352698e-07,
                  2.46518074e-09,  1.28559690e-09],
                [-7.30726592e-08, -9.02886579e-09, -4.92671620e-05,
                  2.04342811e-03, -1.97251965e-01,  7.50329766e-05,
                  5.91706127e-01,  1.88284005e-06, -1.26672432e-07,
                  1.95019815e-07,  2.57056049e-05, -1.93148960e-07,
                  6.98201610e-06, -7.41535970e-01, -4.36895238e-05,
                  2.47175527e-01,  1.40919936e-09, -1.26892193e-07,
                  2.35802907e-09,  3.66011998e-10]])
\end{Verbatim}
            
    On peut voir d'après la covariance matrix que si on garde juste les 4
premières features, on explique 100\% de la variance.

Ça veut tu dire quon peut juste garer les 4 premieres feat et on va
avoir le meme R2 score?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZpc{}}\PY{l+s+s1}{ Variance Explained}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{} of Features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{PCA Analysis}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{30}\PY{p}{,}\PY{l+m+mf}{100.5}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{context}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seaborn\PYZhy{}whitegrid}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{var}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} [<matplotlib.lines.Line2D at 0x7fa779593320>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_97_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{} “””}
         \PY{c+c1}{\PYZsh{} Dimensionality Reduction using PCA (Principal Component Analysis) Here n\PYZus{}components = 2 means, transform into a 2\PYZhy{}Dimensional dataset.}
         \PY{c+c1}{\PYZsh{} “””}
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}pca} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df\PYZus{}testtime\PYZus{}filtered}\PY{p}{)}
         
         
         \PY{n}{principalDf} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}pca}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{principal component 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{principal component 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{principalDf}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x1b1bf4c9f28>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_98_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} “””}
         \PY{c+c1}{\PYZsh{} Dimensionality Reduction using PCA (Principal Component Analysis) Here n\PYZus{}components = 2 means, transform into a 2\PYZhy{}Dimensional dataset.}
         \PY{c+c1}{\PYZsh{} “””}
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}pca} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{df\PYZus{}filtered\PYZus{}3\PYZus{}per\PYZus{}subject}\PY{p}{)}
         
         
         \PY{n}{principalDf} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}pca}\PY{p}{,} \PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{principal component 1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{principal component 2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{principalDf}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}33}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fa75f0fa5c0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    FIXME: le graph me dit pas grand-chose... J'ai l'impression que c'est
parce qu'on a trop de points (+5000) alors ça fait trop un graphique
chargé au final. Il faudrait genre utiliser juste 3 points par patients
au lieu de tous leurs points 

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{\PYZsh{}Moyenne des enregistrement des sujets}
         \PY{l+s+sd}{Mean=df.groupby(\PYZsq{}subject\PYZus{}id\PYZsq{},as\PYZus{}index=False, ).mean()}
         
         \PY{l+s+sd}{\PYZsh{}Retirer les données non désirées}
         \PY{l+s+sd}{X=Mean.drop(columns=[\PYZsq{}motor\PYZus{}UPDRS\PYZsq{},\PYZsq{}total\PYZus{}UPDRS\PYZsq{},\PYZsq{}subject\PYZus{}id\PYZsq{}])}
         
         \PY{l+s+sd}{X\PYZus{}train, X\PYZus{}test, y\PYZus{}train, y\PYZus{}test = get\PYZus{}dataset(None,None)}
         
         \PY{l+s+sd}{\PYZsh{}Application PCA}
         \PY{l+s+sd}{pca = PCA(n\PYZus{}components=2)}
         \PY{l+s+sd}{principalComponents = pca.fit\PYZus{}transform(X)}
         
         \PY{l+s+sd}{x\PYZus{}train\PYZus{}flat = X\PYZus{}train.reshape(\PYZhy{}1,3072)}
         
         \PY{l+s+sd}{df\PYZus{}cifar = pd.DataFrame(x\PYZus{}train\PYZus{}flat,columns=feat\PYZus{}cols)}
         
         \PY{l+s+sd}{pca\PYZus{}cifar = PCA(n\PYZus{}components=2)}
         \PY{l+s+sd}{principalComponents\PYZus{}cifar = pca\PYZus{}cifar.fit\PYZus{}transform(df\PYZus{}cifar.iloc[:,:\PYZhy{}1])}
         
         \PY{l+s+sd}{principal\PYZus{}cifar\PYZus{}Df = pd.DataFrame(data = principalComponents\PYZus{}cifar}
         \PY{l+s+sd}{             , columns = [\PYZsq{}principal component 1\PYZsq{}, \PYZsq{}principal component 2\PYZsq{}])}
         \PY{l+s+sd}{principal\PYZus{}cifar\PYZus{}Df[\PYZsq{}y\PYZsq{}] = y\PYZus{}train}
         
         
         \PY{l+s+sd}{plt.figure(figsize=(16,10))}
         \PY{l+s+sd}{sns.scatterplot(}
         \PY{l+s+sd}{    x=\PYZdq{}principal component 1\PYZdq{}, y=\PYZdq{}principal component 2\PYZdq{},}
         \PY{l+s+sd}{    hue=\PYZdq{}y\PYZdq{},}
         \PY{l+s+sd}{    palette=sns.color\PYZus{}palette(\PYZdq{}hls\PYZdq{}, 10),}
         \PY{l+s+sd}{    data=principal\PYZus{}cifar\PYZus{}Df,}
         \PY{l+s+sd}{    legend=\PYZdq{}full\PYZdq{},}
         \PY{l+s+sd}{    alpha=0.3}
         \PY{l+s+sd}{)}
         \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} '\textbackslash{}n\#Moyenne des enregistrement des sujets\textbackslash{}nMean=df.groupby(\textbackslash{}'subject\_id\textbackslash{}',as\_index=False, ).mean()\textbackslash{}n\textbackslash{}n\#Retirer les données non désirées\textbackslash{}nX=Mean.drop(columns=[\textbackslash{}'motor\_UPDRS\textbackslash{}',\textbackslash{}'total\_UPDRS\textbackslash{}',\textbackslash{}'subject\_id\textbackslash{}'])\textbackslash{}n\textbackslash{}nX\_train, X\_test, y\_train, y\_test = get\_dataset(None,None)\textbackslash{}n\textbackslash{}n\#Application PCA\textbackslash{}npca = PCA(n\_components=2)\textbackslash{}nprincipalComponents = pca.fit\_transform(X)\textbackslash{}n\textbackslash{}nx\_train\_flat = X\_train.reshape(-1,3072)\textbackslash{}n\textbackslash{}ndf\_cifar = pd.DataFrame(x\_train\_flat,columns=feat\_cols)\textbackslash{}n\textbackslash{}npca\_cifar = PCA(n\_components=2)\textbackslash{}nprincipalComponents\_cifar = pca\_cifar.fit\_transform(df\_cifar.iloc[:,:-1])\textbackslash{}n\textbackslash{}nprincipal\_cifar\_Df = pd.DataFrame(data = principalComponents\_cifar\textbackslash{}n             , columns = [\textbackslash{}'principal component 1\textbackslash{}', \textbackslash{}'principal component 2\textbackslash{}'])\textbackslash{}nprincipal\_cifar\_Df[\textbackslash{}'y\textbackslash{}'] = y\_train\textbackslash{}n\textbackslash{}n\textbackslash{}nplt.figure(figsize=(16,10))\textbackslash{}nsns.scatterplot(\textbackslash{}n    x="principal component 1", y="principal component 2",\textbackslash{}n    hue="y",\textbackslash{}n    palette=sns.color\_palette("hls", 10),\textbackslash{}n    data=principal\_cifar\_Df,\textbackslash{}n    legend="full",\textbackslash{}n    alpha=0.3\textbackslash{}n)\textbackslash{}n'
\end{Verbatim}
            
    \section{Multilayer Perceptron}\label{multilayer-perceptron}

    TODO Félix: Faire comme moi avec input scaler, unscaled input,
normalized, standardized etc.

TODO Félix: Faire le graphique loss par epoch / training vs test MSE
comme dans le site web.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{c+c1}{\PYZsh{} fit and evaluate mse of model on test set}
        \PY{k}{def} \PY{n+nf}{evaluate\PYZus{}model}\PY{p}{(}\PY{n}{trainX}\PY{p}{,} \PY{n}{trainy}\PY{p}{,} \PY{n}{testX}\PY{p}{,} \PY{n}{testy}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} define model}
            \PY{n}{model} \PY{o}{=} \PY{n}{Sequential}\PY{p}{(}\PY{p}{)}
            \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{input\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{relu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kernel\PYZus{}initializer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{he\PYZus{}uniform}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{n}{model}\PY{o}{.}\PY{n}{add}\PY{p}{(}\PY{n}{Dense}\PY{p}{(}\PY{n}{output\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{activation}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{linear}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} compile model}
            \PY{n}{model}\PY{o}{.}\PY{n}{compile}\PY{p}{(}\PY{n}{loss}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{n}{SGD}\PY{p}{(}\PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} fit model}
            \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{trainX}\PY{p}{,} \PY{n}{trainy}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{200}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} evaluate the model}
            \PY{n}{test\PYZus{}mse} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{evaluate}\PY{p}{(}\PY{n}{testX}\PY{p}{,} \PY{n}{testy}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
            \PY{k}{return} \PY{n}{test\PYZus{}mse}
         
        \PY{c+c1}{\PYZsh{} evaluate model multiple times with given input and output scalers}
        \PY{k}{def} \PY{n+nf}{repeated\PYZus{}evaluation}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{,} \PY{n}{n\PYZus{}repeats}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} get dataset}
            \PY{n}{trainX}\PY{p}{,} \PY{n}{testX}\PY{p}{,} \PY{n}{trainy}\PY{p}{,} \PY{n}{testy} \PY{o}{=} \PY{n}{get\PYZus{}dataset}\PY{p}{(}\PY{n}{input\PYZus{}scaler}\PY{p}{,} \PY{n}{output\PYZus{}scaler}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} repeated evaluation of model}
            \PY{n}{results} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{p}{)}
            \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n\PYZus{}repeats}\PY{p}{)}\PY{p}{:}
                \PY{n}{test\PYZus{}mse} \PY{o}{=} \PY{n}{evaluate\PYZus{}model}\PY{p}{(}\PY{n}{trainX}\PY{p}{,} \PY{n}{trainy}\PY{p}{,} \PY{n}{testX}\PY{p}{,} \PY{n}{testy}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZgt{}}\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{test\PYZus{}mse}\PY{p}{)}
                \PY{n}{results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{test\PYZus{}mse}\PY{p}{)}
            \PY{k}{return} \PY{n}{results}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} unscaled inputs}
        \PY{n}{results\PYZus{}unscaled\PYZus{}inputs} \PY{o}{=} \PY{n}{repeated\PYZus{}evaluation}\PY{p}{(}\PY{k+kc}{None}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} normalized inputs}
        \PY{n}{results\PYZus{}normalized\PYZus{}inputs} \PY{o}{=} \PY{n}{repeated\PYZus{}evaluation}\PY{p}{(}\PY{n}{MinMaxScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} standardized inputs}
        \PY{n}{results\PYZus{}standardized\PYZus{}inputs} \PY{o}{=} \PY{n}{repeated\PYZus{}evaluation}\PY{p}{(}\PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} summarize results}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unscaled: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{ (}\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{mean}\PY{p}{(}\PY{n}{results\PYZus{}unscaled\PYZus{}inputs}\PY{p}{)}\PY{p}{,} \PY{n}{std}\PY{p}{(}\PY{n}{results\PYZus{}unscaled\PYZus{}inputs}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Normalized: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{ (}\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{mean}\PY{p}{(}\PY{n}{results\PYZus{}normalized\PYZus{}inputs}\PY{p}{)}\PY{p}{,} \PY{n}{std}\PY{p}{(}\PY{n}{results\PYZus{}normalized\PYZus{}inputs}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standardized: }\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{ (}\PY{l+s+si}{\PYZpc{}.3f}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{mean}\PY{p}{(}\PY{n}{results\PYZus{}standardized\PYZus{}inputs}\PY{p}{)}\PY{p}{,} \PY{n}{std}\PY{p}{(}\PY{n}{results\PYZus{}standardized\PYZus{}inputs}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} plot results}
        \PY{n}{results} \PY{o}{=} \PY{p}{[}\PY{n}{results\PYZus{}unscaled\PYZus{}inputs}\PY{p}{,} \PY{n}{results\PYZus{}normalized\PYZus{}inputs}\PY{p}{,} \PY{n}{results\PYZus{}standardized\PYZus{}inputs}\PY{p}{]}
        \PY{n}{labels} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{unscaled}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{normalized}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{standardized}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n}{pyplot}\PY{o}{.}\PY{n}{boxplot}\PY{p}{(}\PY{n}{results}\PY{p}{,} \PY{n}{labels}\PY{o}{=}\PY{n}{labels}\PY{p}{)}
        \PY{n}{pyplot}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation="linear", units=1)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
WARNING:tensorflow:From C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}keras\textbackslash{}backend\textbackslash{}tensorflow\_backend.py:422: The name tf.global\_variables is deprecated. Please use tf.compat.v1.global\_variables instead.

>1.063
>1.064
>1.067
>1.064
>1.064
>1.074
>1.063
>1.064
>1.073
>1.064
>1.065
>1.063
>1.074
>1.063
>1.063
>nan
>1.066
>1.064
>1.091
>1.063
>1.064
>1.065
>1.066
>1.063
>1.066
>1.068
>1.067
>nan
>1.069
>1.064

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation="linear", units=1)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
>0.188
>0.176
>0.192
>0.170
>0.297
>0.148
>0.137
>0.151
>0.132
>0.138
>0.194
>0.114
>0.142
>0.137
>0.160
>0.137
>0.172
>0.105
>0.224
>0.166
>0.149
>0.153
>0.136
>0.155
>0.145
>0.147
>0.121
>0.169
>0.183
>0.139

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}ipykernel\_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation="linear", units=1)`
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
>0.118
>0.122
>0.124
>0.125
>0.152
>0.087
>0.111
>0.122
>0.120
>0.140
>0.105
>0.198
>0.114
>0.110
>0.099
>0.298
>0.120
>0.128
>0.099
>0.224
>0.129
>0.106
>0.121
>0.142
>0.087
>0.108
>0.144
>0.128
>0.146
>0.116
Unscaled: nan (nan)
Normalized: 0.159 (0.036)
Standardized: 0.131 (0.042)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        NameError                                 Traceback (most recent call last)

        <ipython-input-7-9043e6bc9f5b> in <module>()
         12 results = [results\_unscaled\_inputs, results\_normalized\_inputs, results\_standardized\_inputs]
         13 labels = ['unscaled', 'normalized', 'standardized']
    ---> 14 pyplot.boxplot(results, labels=labels)
         15 pyplot.show()
    

        NameError: name 'pyplot' is not defined

    \end{Verbatim}

    \subsection{Évaluation des
résultats}\label{uxe9valuation-des-ruxe9sultats}

    TODO

    \subsubsection{Questions de recherche}\label{questions-de-recherche}

\begin{itemize}
\tightlist
\item
  Y a-t-il un lien entre l'âge du patient et le résultat aux tests?
  Hypothèse: Il devrait effectivement y avoir un lien, assumant que les
  patients plus âgés ont un score plus élevé au test.
\end{itemize}

    \subsection{References}\label{references}

    {[}1{]} Tsanas, A., Little, M. A., McSharry, P. E., \& Ramig, L. O.
(2009). Accurate telemonitoring of Parkinson's disease progression by
noninvasive speech tests. IEEE transactions on Biomedical Engineering,
57(4), 884-893.

{[}2{]} Goetz, C. G., Stebbins, G. T., Wolff, D., DeLeeuw, W.,
Bronte‐Stewart, H., Elble, R., ... \& Wu, A. D. (2009). Testing
objective measures of motor impairment in early Parkinson's disease:
Feasibility study of an at‐home testing device. Movement Disorders,
24(4), 551-556.

    \section{Drafts}\label{drafts}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{14}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}
         \PY{n}{cols} \PY{o}{=} \PY{l+m+mi}{3}
         \PY{n}{rows} \PY{o}{=} \PY{l+m+mi}{3}
         
         \PY{n}{fig2}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{rows}\PY{p}{,} \PY{n}{cols}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{n}{figsize}\PY{p}{,} \PY{n}{constrained\PYZus{}layout}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{axs} \PY{o}{=} \PY{n}{trim\PYZus{}axs}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{l+m+mi}{42}\PY{p}{)}
         \PY{k}{for} \PY{n}{ax}\PY{p}{,} \PY{n}{subject\PYZus{}id} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{axs}\PY{p}{,} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{34}\PY{p}{,}\PY{l+m+mi}{43}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id =}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n+nb}{str}\PY{p}{(}\PY{n}{subject\PYZus{}id}\PY{p}{)}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test\PYZus{}time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}xscale(\PYZsq{}log\PYZsq{})}
         \PY{c+c1}{\PYZsh{}     ax.set\PYZus{}yscale(\PYZsq{}log\PYZsq{})}
             \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{test\PYZus{}time}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n}{subject\PYZus{}id}\PY{p}{]}\PY{o}{.}\PY{n}{motor\PYZus{}UPDRS}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_112_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Pairplot}\label{pairplot}

Source :
https://medium.com/towards-artificial-intelligence/feature-selection-and-dimensionality-reduction-using-covariance-matrix-plot-b4c7498abd07

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} 
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt} 
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        
        
        \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{df}\PY{p}{[}\PY{n}{df}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mf}{2.0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        NameError                                 Traceback (most recent call last)

        <ipython-input-2-88cbcceb969f> in <module>()
          5 
          6 
    ----> 7 sns.pairplot(df[df.keys()], size=2.0)
    

        NameError: name 'df' is not defined

    \end{Verbatim}

    \subsubsection{Pairplot}\label{pairplot}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{c+c1}{\PYZsh{}Moyenne des enregistrement des sujets}
         \PY{n}{Mean}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{as\PYZus{}index}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Retirer les données non désirées}
         \PY{n}{X}\PY{o}{=}\PY{n}{Mean}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{motor\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{total\PYZus{}UPDRS}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{subject\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{StandardScaler} 
         \PY{n}{stdsc} \PY{o}{=} \PY{n}{StandardScaler}\PY{p}{(}\PY{p}{)} 
         \PY{n}{X\PYZus{}std} \PY{o}{=} \PY{n}{stdsc}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n}{cov\PYZus{}mat} \PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{cov}\PY{p}{(}\PY{n}{X\PYZus{}std}\PY{o}{.}\PY{n}{T}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{font\PYZus{}scale}\PY{o}{=}\PY{l+m+mf}{1.5}\PY{p}{)}
         \PY{n}{hm} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{cov\PYZus{}mat}\PY{p}{,}
                          \PY{n}{cbar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                          \PY{n}{annot}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                          \PY{n}{square}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                          \PY{n}{fmt}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.2f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                          \PY{n}{annot\PYZus{}kws}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{12}\PY{p}{\PYZcb{}}\PY{p}{,}
                          \PY{n}{cmap}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coolwarm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}                 
                          \PY{n}{yticklabels}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{,}
                          \PY{n}{xticklabels}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Covariance matrix showing correlation coefficients}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size} \PY{o}{=} \PY{l+m+mi}{18}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_116_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
